{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XHSpY66kApA",
        "outputId": "dca7b586-f665-4e54-d0f9-1e3937ba3d15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: fairseq in /usr/local/lib/python3.10/dist-packages (0.12.2)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.15.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq) (0.29.34)\n",
            "Requirement already satisfied: hydra-core<1.1,>=1.0.7 in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.0.7)\n",
            "Requirement already satisfied: omegaconf<2.1 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.0.6)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq) (2022.10.31)\n",
            "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.3.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq) (4.65.0)\n",
            "Requirement already satisfied: bitarray in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.7.3)\n",
            "Requirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.0.2+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.22.4)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.10/dist-packages (from hydra-core<1.1,>=1.0.7->fairseq) (4.8)\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (4.5.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (2.7.0)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.8.10)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (4.9.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.12.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->fairseq) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->fairseq) (16.0.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq) (2.21)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->fairseq) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->fairseq) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: fastbpe in /usr/local/lib/python3.10/dist-packages (0.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: vncorenlp in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vncorenlp) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install fairseq\n",
        "!pip3 install fastbpe\n",
        "!pip3 install vncorenlp\n",
        "!pip3 install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBhRKe5xkHuf",
        "outputId": "a793f8a1-8065-4b04-abac-b7fc7bc9f020"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-21 14:22:29--  https://public.vinai.io/PhoBERT_base_fairseq.tar.gz\n",
            "Resolving public.vinai.io (public.vinai.io)... 13.227.219.99, 13.227.219.93, 13.227.219.12, ...\n",
            "Connecting to public.vinai.io (public.vinai.io)|13.227.219.99|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1243308020 (1.2G) [application/x-tar]\n",
            "Saving to: ‘PhoBERT_base_fairseq.tar.gz.1’\n",
            "\n",
            "PhoBERT_base_fairse 100%[===================>]   1.16G   228MB/s    in 4.9s    \n",
            "\n",
            "2023-05-21 14:22:35 (241 MB/s) - ‘PhoBERT_base_fairseq.tar.gz.1’ saved [1243308020/1243308020]\n",
            "\n",
            "PhoBERT_base_fairseq/\n",
            "PhoBERT_base_fairseq/bpe.codes\n",
            "PhoBERT_base_fairseq/model.pt\n",
            "PhoBERT_base_fairseq/dict.txt\n"
          ]
        }
      ],
      "source": [
        "!wget https://public.vinai.io/PhoBERT_base_fairseq.tar.gz\n",
        "!tar -xzvf PhoBERT_base_fairseq.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weHISCg-kKId",
        "outputId": "69c913fe-b078-42a8-f2d0-deebffb053cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bpe.codes  dict.txt  model.pt\n"
          ]
        }
      ],
      "source": [
        "!ls PhoBERT_base_fairseq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfILmHy_lPxn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3257ac98-bf94-4cb8-be12-4ceadf5a03d2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaHubInterface(\n",
              "  (model): RobertaModel(\n",
              "    (encoder): RobertaEncoder(\n",
              "      (sentence_encoder): TransformerEncoder(\n",
              "        (dropout_module): FairseqDropout()\n",
              "        (embed_tokens): Embedding(64001, 768, padding_idx=1)\n",
              "        (embed_positions): LearnedPositionalEmbedding(258, 768, padding_idx=1)\n",
              "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (layers): ModuleList(\n",
              "          (0-11): 12 x TransformerEncoderLayerBase(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (lm_head): RobertaLMHead(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "    (classification_heads): ModuleDict()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# Load the model in fairseq\n",
        "from fairseq.models.roberta import RobertaModel\n",
        "phoBERT = RobertaModel.from_pretrained('PhoBERT_base_fairseq', checkpoint_file='model.pt')\n",
        "phoBERT.eval()  # disable dropout (or leave in train mode to finetune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCUwR-n-ksd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "644dd766-178b-4b0a-8431-dad67e7d21ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens list :  tensor([   0,  622, 2919,  222, 2054,  222,  385, 8249, 1696, 7657,    2])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Trường đại học khoa học tự nhiên Hà Nội'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "from fairseq.data.encoders.fastbpe import fastBPE\n",
        "\n",
        "class BPE():\n",
        "  bpe_codes = 'PhoBERT_base_fairseq/bpe.codes'\n",
        "\n",
        "args = BPE()\n",
        "phoBERT.bpe = fastBPE(args)\n",
        "tokens = phoBERT.encode('Trường đại học khoa học tự nhiên Hà Nội')\n",
        "print('tokens list : ', tokens)\n",
        "# Decode ngược lại thành câu từ chuỗi index token\n",
        "phoBERT.decode(tokens)  # 'Hello world!'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2pWopbSkudG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8284c2f2-4372-4186-ac25-97efb3983dbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token size:  torch.Size([11])\n",
            "size of last layer:  torch.Size([1, 11, 768])\n",
            "number layer in all layers:  13\n",
            "Last layer features:  tensor([[[True, True, True,  ..., True, True, True],\n",
            "         [True, True, True,  ..., True, True, True],\n",
            "         [True, True, True,  ..., True, True, True],\n",
            "         ...,\n",
            "         [True, True, True,  ..., True, True, True],\n",
            "         [True, True, True,  ..., True, True, True],\n",
            "         [True, True, True,  ..., True, True, True]]])\n"
          ]
        }
      ],
      "source": [
        "# Trích xuất đặt trưng từ lớp cuối cùng\n",
        "last_layer_features = phoBERT.extract_features(tokens)\n",
        "\n",
        "print('token size: ', tokens.size())\n",
        "print('size of last layer: ', last_layer_features.size())\n",
        "\n",
        "# Extract all layer's features (layer 0 is the embedding layer)\n",
        "all_layers = phoBERT.extract_features(tokens, return_all_hiddens=True)\n",
        "print('number layer in all layers: ', len(all_layers))\n",
        "\n",
        "# Ma trận vector đặc trưng của layer cuối bằng ma trận vector đặc trưng lấy ra\n",
        "# từ all layer\n",
        "print('Last layer features: ', all_layers[-1] == last_layer_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWeACJgYkvyv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "876a7e4f-5b0c-40ba-dcdf-2a21b238482a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-21 14:23:12--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 27412575 (26M) [application/octet-stream]\n",
            "Saving to: ‘VnCoreNLP-1.1.1.jar’\n",
            "\n",
            "\rVnCoreNLP-1.1.1.jar   0%[                    ]       0  --.-KB/s               \rVnCoreNLP-1.1.1.jar 100%[===================>]  26.14M   173MB/s    in 0.2s    \n",
            "\n",
            "2023-05-21 14:23:12 (173 MB/s) - ‘VnCoreNLP-1.1.1.jar’ saved [27412575/27412575]\n",
            "\n",
            "--2023-05-21 14:23:12--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 526544 (514K) [application/octet-stream]\n",
            "Saving to: ‘vi-vocab’\n",
            "\n",
            "vi-vocab            100%[===================>] 514.20K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-05-21 14:23:12 (24.5 MB/s) - ‘vi-vocab’ saved [526544/526544]\n",
            "\n",
            "--2023-05-21 14:23:12--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 128508 (125K) [text/plain]\n",
            "Saving to: ‘wordsegmenter.rdr’\n",
            "\n",
            "wordsegmenter.rdr   100%[===================>] 125.50K  --.-KB/s    in 0.007s  \n",
            "\n",
            "2023-05-21 14:23:13 (16.9 MB/s) - ‘wordsegmenter.rdr’ saved [128508/128508]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download VnCoreNLP-1.1.1.jar & its word segmentation component (i.e. RDRSegmenter)\n",
        "!mkdir -p vncorenlp/models/wordsegmenter\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n",
        "!mv VnCoreNLP-1.1.1.jar vncorenlp/\n",
        "!mv vi-vocab vncorenlp/models/wordsegmenter/\n",
        "!mv wordsegmenter.rdr vncorenlp/models/wordsegmenter/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mog68R2kxWx"
      },
      "outputs": [],
      "source": [
        "from vncorenlp import VnCoreNLP\n",
        "rdrsegmenter = VnCoreNLP(\"vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i406SewlkyXS"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "from fairseq.data import Dictionary\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CG-AtA-NkznZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94a14b36-aeb3-4f7f-9e07-1b7c7b061f49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 761.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x1 tensor encode: [63117  1302   884  5958    11   915   222   537   933    39], shape: 256\n",
            "x1 tensor decode:  <s> Học_sinh được nghỉ học bắt dầu từ tháng 3 để tránh dịch covid-19 </s> <pad> <pad> <pad> <pad> <pad>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "max_sequence_length = 256\n",
        "def convert_lines(lines, vocab, bpe):\n",
        "  '''\n",
        "  lines: list các văn bản input\n",
        "  vocab: từ điển dùng để encoding subwords\n",
        "  bpe:\n",
        "  '''\n",
        "  # Khởi tạo ma trận output\n",
        "  outputs = np.zeros((len(lines), max_sequence_length), dtype=np.int32) # --> shape (number_lines, max_seq_len)\n",
        "  # Index của các token cls (đầu câu), eos (cuối câu), padding (padding token)\n",
        "  cls_id = 0\n",
        "  eos_id = 2\n",
        "  pad_id = 1\n",
        "\n",
        "  for idx, row in tqdm(enumerate(lines), total=len(lines)):\n",
        "    # Mã hóa subwords theo byte pair encoding(bpe)\n",
        "    subwords = bpe.encode('<s> '+ row +' </s>')\n",
        "    input_ids = vocab.encode_line(subwords, append_eos=False, add_if_not_exist=False).long().tolist()\n",
        "    # Truncate input nếu độ dài vượt quá max_seq_len\n",
        "    if len(input_ids) > max_sequence_length:\n",
        "      input_ids = input_ids[:max_sequence_length]\n",
        "      input_ids[-1] = eos_id\n",
        "    else:\n",
        "      # Padding nếu độ dài câu chưa bằng max_seq_len\n",
        "      input_ids = input_ids + [pad_id, ]*(max_sequence_length - len(input_ids))\n",
        "\n",
        "    outputs[idx,:] = np.array(input_ids)\n",
        "  return outputs\n",
        "\n",
        "# Load the dictionary\n",
        "vocab = Dictionary()\n",
        "vocab.add_from_file(\"/content/PhoBERT_base_fairseq/dict.txt\")\n",
        "\n",
        "\n",
        "\n",
        "# Test encode lines\n",
        "lines = ['Học_sinh được nghỉ học bắt dầu từ tháng 3 để tránh dịch covid-19', 'số lượng ca nhiễm bệnh đã giảm bắt đầu từ tháng 5 nhờ biện pháp mạnh tay']\n",
        "[x1, x2] = convert_lines(lines, vocab, phoBERT.bpe)\n",
        "print('x1 tensor encode: {}, shape: {}'.format(x1[:10], x1.size))\n",
        "print('x1 tensor decode: ', phoBERT.decode(torch.tensor(x1))[:103])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQEGQn7PuZZV"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import chardet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rXqrm6Zk1un"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load data from text file\n",
        "with open('data.json', 'r') as f:\n",
        "    data = [json.loads(line) for line in f]\n",
        "\n",
        "# Split data into train and test\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Save train data to JSON file\n",
        "with open('train.json', 'w') as f:\n",
        "    for item in train_data:\n",
        "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
        "\n",
        "# Save test data to JSON file\n",
        "with open('test.json', 'w') as f:\n",
        "    for item in test_data:\n",
        "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfxlDgtWuT4o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90b45611-a427-44ca-b337-b46fb393f120"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['công tác quản lý xử lý vi phạm hành chính xlvphc và theo dõi thi hành pháp luật được đẩy mạnh thực hiện , đảm bảo sự gắn kết chặt chẽ giữa thi hành pháp luật với xây dựn pháp luật .', 'Tuy nhiên, vì lý do cá nhân nên cha của Linh cũng không chuộc được xe.', 'google cũng yêu cầu tòa giữ kín một số thông tin thiệt hại mà google tiết lộ trong một đơn kiện gửi tòa .', 'Cháu đòi tiền cơm, dì đòi tiền nhà.', 'Luật sư VŨ PHI LONG , nguyên Phó Chánh Tòa Hình sự TAND TP.HCM PHƯƠNG LOAN']\n",
            "[False, True, False, True, True]\n"
          ]
        }
      ],
      "source": [
        "# Đọc lại file với encoding tương ứng\n",
        "with open('train.json' ,'r') as f:\n",
        "    train = f.read()\n",
        "# Chuyển nội dung thành danh sách các đối tượng\n",
        "# json.loads(và json.load) không giải mã nhiều đối tượng json.\n",
        "# Kết xuất JSON của data.json có một đối tượng trên mỗi dòng\n",
        "objects = [json.loads(line) for line in open('train.json', 'r')]\n",
        "\n",
        "# Lấy giá trị của thuộc tính sentence trong từng đối tượng\n",
        "sentences = []\n",
        "for obj in objects:\n",
        "    sentence = obj['sentence']\n",
        "    sentences.append(sentence)\n",
        "\n",
        "labels = []\n",
        "for obj in objects:\n",
        "    label = obj['labels']\n",
        "    labels.append(label)\n",
        "\n",
        "# In ra 5 giá trị đầu của sentence và label\n",
        "print(sentences[:5])\n",
        "print(labels[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjH_7I7yk29Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "285c8f3e-5975-42f7-8c6e-b70271b25f56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[False  True]\n",
            "[0 1 0 1 1]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "lb = LabelEncoder()\n",
        "lb.fit(labels)\n",
        "y = lb.fit_transform(labels)\n",
        "print(lb.classes_)\n",
        "# print('Top 5 classes indices: ', y[:5])\n",
        "print(y[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhXrLEWuk394"
      },
      "outputs": [],
      "source": [
        "X = sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVZ3TX1ak5Qe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d66daca3-0b1b-400c-b5df-8182acb1ecd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12000/12000 [00:05<00:00, 2273.01it/s]\n"
          ]
        }
      ],
      "source": [
        "X = convert_lines(X, vocab, phoBERT.bpe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqTHUHK1k6L4"
      },
      "outputs": [],
      "source": [
        "phoBERT_cls = RobertaModel.from_pretrained('PhoBERT_base_fairseq', checkpoint_file='model.pt')\n",
        "phoBERT_cls.eval()  # disable dropout (or leave in train mode to finetune\n",
        "phoBERT_cls.bpe = fastBPE(args) #Incorporate the BPE encoder into PhoBERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4htqS837k7JU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be6686d0-2bf6-468f-81a3-186bfe046731"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.7590, -0.6314]], grad_fn=<LogSoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "phoBERT_cls.register_classification_head('new_task', num_classes=2)\n",
        "tokens = sentences[0]\n",
        "token_idxs = phoBERT_cls.encode(tokens)\n",
        "logprobs = phoBERT_cls.predict('new_task', token_idxs)  # tensor([[-1.1050, -1.0672, -1.1245]], grad_fn=<LogSoftmaxBackward>)\n",
        "logprobs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVV0doj-k8F1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e548943a-53ef-4158-c8e0-8ddc63c7838d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1])\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "with torch.no_grad():\n",
        "  x = np.argmax(logprobs, axis = 1)\n",
        "  print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jurSSMcMk9EX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d26610e5-4aea-479b-e2dd-83a62fa5560f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 0.0)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "def evaluate(logits, targets):\n",
        "    \"\"\"\n",
        "    Đánh giá model sử dụng accuracy và f1 scores.\n",
        "    Args:\n",
        "        logits (B,C): torch.LongTensor. giá trị predicted logit cho class output.\n",
        "        targets (B): torch.LongTensor. actual target indices.\n",
        "    Returns:\n",
        "        acc (float): the accuracy score\n",
        "        f1 (float): the f1 score\n",
        "    \"\"\"\n",
        "    # Tính accuracy score và f1_score\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    y_pred = np.argmax(logits, axis = 1)\n",
        "    targets = targets.detach().cpu().numpy()\n",
        "    f1 = f1_score(targets, y_pred, average='weighted')\n",
        "    acc = accuracy_score(targets, y_pred)\n",
        "    return acc, f1\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "logits = torch.tensor([[0.1, 0.2, 0.7],\n",
        "                       [0.4, 0.1, 0.5],\n",
        "                       [0.1, 0.2, 0.7]]).to(device)\n",
        "targets = torch.tensor([0, 1, 1]).to(device)\n",
        "evaluate(logits, targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KpdC7KWk99D"
      },
      "outputs": [],
      "source": [
        "def validate(valid_loader, model, device):\n",
        "    model.eval()\n",
        "    accs = []\n",
        "    f1s = []\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch in valid_loader:\n",
        "            x_batch = x_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            outputs = model.predict('new_task', x_batch)\n",
        "            logits = torch.exp(outputs)\n",
        "            acc, f1 = evaluate(logits, y_batch)\n",
        "            accs.append(acc)\n",
        "            f1s.append(f1)\n",
        "\n",
        "    mean_acc = np.mean(accs)\n",
        "    mean_f1 = np.mean(f1s)\n",
        "    return mean_acc, mean_f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzw0YcFzk-_X"
      },
      "outputs": [],
      "source": [
        "import time, os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GWhi5Dmk_3P"
      },
      "outputs": [],
      "source": [
        "def trainOnEpoch(train_loader, model, optimizer, epoch, num_epochs, criteria, device, log_aggr = 100):\n",
        "    model.train()\n",
        "    sum_epoch_loss = 0\n",
        "    sum_acc = 0\n",
        "    sum_f1 = 0\n",
        "    start = time.time()\n",
        "    for i, (x_batch, y_batch) in enumerate(train_loader):\n",
        "      x_batch = x_batch.to(device)\n",
        "      y_batch = y_batch.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      y_pred = model.predict('new_task', x_batch)\n",
        "      logits = torch.exp(y_pred)\n",
        "      acc, f1 = evaluate(logits, y_batch)\n",
        "      loss = criteria(y_pred, y_batch)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      loss_val = loss.item()\n",
        "      sum_epoch_loss += loss_val\n",
        "      sum_acc += acc\n",
        "      sum_f1 += f1\n",
        "      iter_num = epoch * len(train_loader) + i + 1\n",
        "\n",
        "      if i % log_aggr == 0:\n",
        "            print('[TRAIN] epoch %d/%d  observation %d/%d batch loss: %.4f (avg %.4f),  avg acc: %.4f, avg f1: %.4f, (%.2f im/s)'\n",
        "                % (epoch + 1, num_epochs, i, len(train_loader), loss_val, sum_epoch_loss / (i + 1),  sum_acc/(i+1), sum_f1/(i+1),\n",
        "                  len(x_batch) / (time.time() - start)))\n",
        "      start = time.time()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVf7PcvvlAzR"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.autograd import Variable\n",
        "from torch.backends import cudnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iFn2ZFNlDZ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e5ddd0d-1189-4c0d-857a-f498e212cea1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation_tf_utils.py:24: FutureWarning: Importing `TFGenerationMixin` from `src/transformers/generation_tf_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import TFGenerationMixin` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation_flax_utils.py:24: FutureWarning: Importing `FlaxGenerationMixin` from `src/transformers/generation_flax_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import FlaxGenerationMixin` instead.\n",
            "  warnings.warn(\n",
            "Xformers is not installed correctly. If you want to use memorry_efficient_attention to accelerate training use the following command to install Xformers\n",
            "pip install xformers.\n"
          ]
        }
      ],
      "source": [
        "from transformers.modeling_utils import *\n",
        "from transformers import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jwuv1dsClET4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4de2c2c-0c8a-47c2-abe6-102bc574da68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 1291M\n",
            "drwxrwxr-x 2 1017 1025    1M Mar  2  2020 .\n",
            "drwxr-xr-x 1 root root    1M May 19 20:21 ..\n",
            "-rw-rw-r-- 1 1017 1025    2M Mar  2  2020 bpe.codes\n",
            "-rw-rw-r-- 1 1017 1025    1M Mar  2  2020 dict.txt\n",
            "-rw-rw-r-- 1 1017 1025 1289M Jan 20  2020 model.pt\n"
          ]
        }
      ],
      "source": [
        "!ls -la --block-size=M PhoBERT_base_fairseq/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvouXBeC1NkZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06dffb77-2d05-4a42-c147-9db6aa1b4e6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for fold 4\n",
            "Load model pretrained!\n",
            "Load BPE\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Init Optimizer, scheduler, criteria\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  0\n",
            "[TRAIN] epoch 1/20  observation 0/1600 batch loss: 0.6798 (avg 0.6798),  avg acc: 0.6667, avg f1: 0.6667, (1.81 im/s)\n",
            "[TRAIN] epoch 1/20  observation 100/1600 batch loss: 0.3961 (avg 0.7116),  avg acc: 0.5891, avg f1: 0.4786, (23.34 im/s)\n",
            "[TRAIN] epoch 1/20  observation 200/1600 batch loss: 0.5637 (avg 0.6677),  avg acc: 0.6468, avg f1: 0.5376, (22.92 im/s)\n",
            "[TRAIN] epoch 1/20  observation 300/1600 batch loss: 0.6369 (avg 0.6618),  avg acc: 0.6523, avg f1: 0.5417, (22.32 im/s)\n",
            "[TRAIN] epoch 1/20  observation 400/1600 batch loss: 0.5196 (avg 0.6546),  avg acc: 0.6584, avg f1: 0.5468, (22.33 im/s)\n",
            "[TRAIN] epoch 1/20  observation 500/1600 batch loss: 0.7310 (avg 0.6554),  avg acc: 0.6564, avg f1: 0.5429, (22.69 im/s)\n",
            "[TRAIN] epoch 1/20  observation 600/1600 batch loss: 0.5169 (avg 0.6508),  avg acc: 0.6611, avg f1: 0.5480, (22.51 im/s)\n",
            "[TRAIN] epoch 1/20  observation 700/1600 batch loss: 0.8780 (avg 0.6488),  avg acc: 0.6626, avg f1: 0.5489, (22.58 im/s)\n",
            "[TRAIN] epoch 1/20  observation 800/1600 batch loss: 0.5074 (avg 0.6477),  avg acc: 0.6631, avg f1: 0.5499, (22.63 im/s)\n",
            "[TRAIN] epoch 1/20  observation 900/1600 batch loss: 0.6306 (avg 0.6501),  avg acc: 0.6582, avg f1: 0.5432, (22.08 im/s)\n",
            "[TRAIN] epoch 1/20  observation 1000/1600 batch loss: 0.6462 (avg 0.6485),  avg acc: 0.6598, avg f1: 0.5447, (22.47 im/s)\n",
            "[TRAIN] epoch 1/20  observation 1100/1600 batch loss: 0.5366 (avg 0.6469),  avg acc: 0.6615, avg f1: 0.5465, (22.39 im/s)\n",
            "[TRAIN] epoch 1/20  observation 1200/1600 batch loss: 0.8043 (avg 0.6464),  avg acc: 0.6621, avg f1: 0.5468, (22.66 im/s)\n",
            "[TRAIN] epoch 1/20  observation 1300/1600 batch loss: 0.5134 (avg 0.6464),  avg acc: 0.6615, avg f1: 0.5459, (22.31 im/s)\n",
            "[TRAIN] epoch 1/20  observation 1400/1600 batch loss: 0.6380 (avg 0.6459),  avg acc: 0.6620, avg f1: 0.5464, (22.39 im/s)\n",
            "[TRAIN] epoch 1/20  observation 1500/1600 batch loss: 0.6342 (avg 0.6455),  avg acc: 0.6626, avg f1: 0.5472, (22.45 im/s)\n",
            "Epoch 0 validation: acc: 0.6675, f1: 0.5522 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  5%|▌         | 1/20 [07:53<2:29:59, 473.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  1\n",
            "[TRAIN] epoch 2/20  observation 0/1600 batch loss: 0.7979 (avg 0.7979),  avg acc: 0.5000, avg f1: 0.3333, (19.63 im/s)\n",
            "[TRAIN] epoch 2/20  observation 100/1600 batch loss: 0.7322 (avg 0.6414),  avg acc: 0.6634, avg f1: 0.5455, (22.09 im/s)\n",
            "[TRAIN] epoch 2/20  observation 200/1600 batch loss: 0.7385 (avg 0.6388),  avg acc: 0.6692, avg f1: 0.5554, (22.39 im/s)\n",
            "[TRAIN] epoch 2/20  observation 300/1600 batch loss: 0.5514 (avg 0.6393),  avg acc: 0.6683, avg f1: 0.5533, (22.64 im/s)\n",
            "[TRAIN] epoch 2/20  observation 400/1600 batch loss: 0.7402 (avg 0.6383),  avg acc: 0.6692, avg f1: 0.5531, (22.47 im/s)\n",
            "[TRAIN] epoch 2/20  observation 500/1600 batch loss: 0.7459 (avg 0.6387),  avg acc: 0.6683, avg f1: 0.5517, (22.37 im/s)\n",
            "[TRAIN] epoch 2/20  observation 600/1600 batch loss: 0.5163 (avg 0.6352),  avg acc: 0.6730, avg f1: 0.5576, (22.37 im/s)\n",
            "[TRAIN] epoch 2/20  observation 700/1600 batch loss: 0.9414 (avg 0.6369),  avg acc: 0.6705, avg f1: 0.5548, (22.73 im/s)\n",
            "[TRAIN] epoch 2/20  observation 800/1600 batch loss: 0.5278 (avg 0.6373),  avg acc: 0.6702, avg f1: 0.5541, (22.54 im/s)\n",
            "[TRAIN] epoch 2/20  observation 900/1600 batch loss: 0.5150 (avg 0.6351),  avg acc: 0.6730, avg f1: 0.5574, (22.57 im/s)\n",
            "[TRAIN] epoch 2/20  observation 1000/1600 batch loss: 0.5265 (avg 0.6348),  avg acc: 0.6732, avg f1: 0.5576, (22.30 im/s)\n",
            "[TRAIN] epoch 2/20  observation 1100/1600 batch loss: 0.5288 (avg 0.6353),  avg acc: 0.6723, avg f1: 0.5569, (22.85 im/s)\n",
            "[TRAIN] epoch 2/20  observation 1200/1600 batch loss: 0.4951 (avg 0.6351),  avg acc: 0.6722, avg f1: 0.5566, (22.45 im/s)\n",
            "[TRAIN] epoch 2/20  observation 1300/1600 batch loss: 0.8240 (avg 0.6365),  avg acc: 0.6703, avg f1: 0.5544, (22.71 im/s)\n",
            "[TRAIN] epoch 2/20  observation 1400/1600 batch loss: 0.7251 (avg 0.6387),  avg acc: 0.6665, avg f1: 0.5500, (22.53 im/s)\n",
            "[TRAIN] epoch 2/20  observation 1500/1600 batch loss: 0.6407 (avg 0.6389),  avg acc: 0.6664, avg f1: 0.5499, (22.23 im/s)\n",
            "Epoch 1 validation: acc: 0.6675, f1: 0.5522 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 2/20 [15:41<2:21:07, 470.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  2\n",
            "[TRAIN] epoch 3/20  observation 0/1600 batch loss: 0.5018 (avg 0.5018),  avg acc: 0.8333, avg f1: 0.7576, (18.61 im/s)\n",
            "[TRAIN] epoch 3/20  observation 100/1600 batch loss: 0.4995 (avg 0.6587),  avg acc: 0.6419, avg f1: 0.5178, (22.05 im/s)\n",
            "[TRAIN] epoch 3/20  observation 200/1600 batch loss: 0.7811 (avg 0.6445),  avg acc: 0.6592, avg f1: 0.5414, (22.16 im/s)\n",
            "[TRAIN] epoch 3/20  observation 300/1600 batch loss: 0.6492 (avg 0.6463),  avg acc: 0.6567, avg f1: 0.5374, (22.60 im/s)\n",
            "[TRAIN] epoch 3/20  observation 400/1600 batch loss: 0.6403 (avg 0.6482),  avg acc: 0.6542, avg f1: 0.5349, (22.37 im/s)\n",
            "[TRAIN] epoch 3/20  observation 500/1600 batch loss: 0.5070 (avg 0.6462),  avg acc: 0.6567, avg f1: 0.5382, (22.54 im/s)\n",
            "[TRAIN] epoch 3/20  observation 600/1600 batch loss: 0.5059 (avg 0.6441),  avg acc: 0.6592, avg f1: 0.5410, (22.82 im/s)\n",
            "[TRAIN] epoch 3/20  observation 700/1600 batch loss: 0.7614 (avg 0.6443),  avg acc: 0.6588, avg f1: 0.5397, (22.62 im/s)\n",
            "[TRAIN] epoch 3/20  observation 800/1600 batch loss: 0.7703 (avg 0.6409),  avg acc: 0.6631, avg f1: 0.5458, (22.43 im/s)\n",
            "[TRAIN] epoch 3/20  observation 900/1600 batch loss: 0.5060 (avg 0.6386),  avg acc: 0.6661, avg f1: 0.5499, (22.68 im/s)\n",
            "[TRAIN] epoch 3/20  observation 1000/1600 batch loss: 0.6416 (avg 0.6385),  avg acc: 0.6662, avg f1: 0.5504, (22.80 im/s)\n",
            "[TRAIN] epoch 3/20  observation 1100/1600 batch loss: 0.6383 (avg 0.6379),  avg acc: 0.6668, avg f1: 0.5510, (22.66 im/s)\n",
            "[TRAIN] epoch 3/20  observation 1200/1600 batch loss: 0.7651 (avg 0.6375),  avg acc: 0.6672, avg f1: 0.5514, (22.40 im/s)\n",
            "[TRAIN] epoch 3/20  observation 1300/1600 batch loss: 1.0179 (avg 0.6364),  avg acc: 0.6686, avg f1: 0.5533, (22.88 im/s)\n",
            "[TRAIN] epoch 3/20  observation 1400/1600 batch loss: 0.5090 (avg 0.6359),  avg acc: 0.6693, avg f1: 0.5541, (22.60 im/s)\n",
            "[TRAIN] epoch 3/20  observation 1500/1600 batch loss: 0.7609 (avg 0.6366),  avg acc: 0.6682, avg f1: 0.5527, (22.55 im/s)\n",
            "Epoch 2 validation: acc: 0.6675, f1: 0.5522 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▌        | 3/20 [23:28<2:12:47, 468.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  3\n",
            "[TRAIN] epoch 4/20  observation 0/1600 batch loss: 0.5096 (avg 0.5096),  avg acc: 0.8333, avg f1: 0.7576, (17.02 im/s)\n",
            "[TRAIN] epoch 4/20  observation 100/1600 batch loss: 0.7601 (avg 0.6476),  avg acc: 0.6535, avg f1: 0.5333, (22.43 im/s)\n",
            "[TRAIN] epoch 4/20  observation 200/1600 batch loss: 0.6353 (avg 0.6313),  avg acc: 0.6750, avg f1: 0.5616, (22.78 im/s)\n",
            "[TRAIN] epoch 4/20  observation 300/1600 batch loss: 0.7696 (avg 0.6292),  avg acc: 0.6777, avg f1: 0.5651, (22.69 im/s)\n",
            "[TRAIN] epoch 4/20  observation 400/1600 batch loss: 0.3861 (avg 0.6318),  avg acc: 0.6741, avg f1: 0.5605, (22.48 im/s)\n",
            "[TRAIN] epoch 4/20  observation 500/1600 batch loss: 0.5129 (avg 0.6319),  avg acc: 0.6737, avg f1: 0.5596, (22.36 im/s)\n",
            "[TRAIN] epoch 4/20  observation 600/1600 batch loss: 0.7554 (avg 0.6357),  avg acc: 0.6683, avg f1: 0.5531, (22.27 im/s)\n",
            "[TRAIN] epoch 4/20  observation 700/1600 batch loss: 0.7582 (avg 0.6390),  avg acc: 0.6638, avg f1: 0.5470, (22.66 im/s)\n",
            "[TRAIN] epoch 4/20  observation 800/1600 batch loss: 0.5231 (avg 0.6404),  avg acc: 0.6619, avg f1: 0.5448, (22.69 im/s)\n",
            "[TRAIN] epoch 4/20  observation 900/1600 batch loss: 0.8650 (avg 0.6379),  avg acc: 0.6652, avg f1: 0.5485, (22.44 im/s)\n",
            "[TRAIN] epoch 4/20  observation 1000/1600 batch loss: 0.5161 (avg 0.6380),  avg acc: 0.6652, avg f1: 0.5490, (22.41 im/s)\n",
            "[TRAIN] epoch 4/20  observation 1100/1600 batch loss: 0.5128 (avg 0.6359),  avg acc: 0.6680, avg f1: 0.5523, (22.60 im/s)\n",
            "[TRAIN] epoch 4/20  observation 1200/1600 batch loss: 0.7525 (avg 0.6370),  avg acc: 0.6664, avg f1: 0.5501, (22.24 im/s)\n",
            "[TRAIN] epoch 4/20  observation 1300/1600 batch loss: 0.6353 (avg 0.6371),  avg acc: 0.6663, avg f1: 0.5501, (22.33 im/s)\n",
            "[TRAIN] epoch 4/20  observation 1400/1600 batch loss: 0.5154 (avg 0.6361),  avg acc: 0.6676, avg f1: 0.5520, (22.17 im/s)\n",
            "[TRAIN] epoch 4/20  observation 1500/1600 batch loss: 0.6371 (avg 0.6358),  avg acc: 0.6682, avg f1: 0.5528, (22.52 im/s)\n",
            "Epoch 3 validation: acc: 0.6675, f1: 0.5522 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 4/20 [31:14<2:04:42, 467.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  4\n",
            "[TRAIN] epoch 5/20  observation 0/1600 batch loss: 0.6315 (avg 0.6315),  avg acc: 0.6667, avg f1: 0.5333, (18.61 im/s)\n",
            "[TRAIN] epoch 5/20  observation 100/1600 batch loss: 0.7524 (avg 0.6397),  avg acc: 0.6617, avg f1: 0.5553, (22.42 im/s)\n",
            "[TRAIN] epoch 5/20  observation 200/1600 batch loss: 0.5162 (avg 0.6346),  avg acc: 0.6692, avg f1: 0.5580, (22.14 im/s)\n",
            "[TRAIN] epoch 5/20  observation 300/1600 batch loss: 0.5182 (avg 0.6379),  avg acc: 0.6645, avg f1: 0.5496, (22.24 im/s)\n",
            "[TRAIN] epoch 5/20  observation 400/1600 batch loss: 0.7481 (avg 0.6399),  avg acc: 0.6617, avg f1: 0.5467, (22.66 im/s)\n",
            "[TRAIN] epoch 5/20  observation 500/1600 batch loss: 0.8718 (avg 0.6396),  avg acc: 0.6620, avg f1: 0.5471, (22.34 im/s)\n",
            "[TRAIN] epoch 5/20  observation 600/1600 batch loss: 0.6345 (avg 0.6345),  avg acc: 0.6694, avg f1: 0.5559, (22.78 im/s)\n",
            "[TRAIN] epoch 5/20  observation 700/1600 batch loss: 0.3924 (avg 0.6318),  avg acc: 0.6733, avg f1: 0.5601, (22.44 im/s)\n",
            "[TRAIN] epoch 5/20  observation 800/1600 batch loss: 0.7608 (avg 0.6334),  avg acc: 0.6712, avg f1: 0.5569, (22.52 im/s)\n",
            "[TRAIN] epoch 5/20  observation 900/1600 batch loss: 0.7526 (avg 0.6350),  avg acc: 0.6689, avg f1: 0.5535, (22.22 im/s)\n",
            "[TRAIN] epoch 5/20  observation 1000/1600 batch loss: 0.5264 (avg 0.6358),  avg acc: 0.6677, avg f1: 0.5516, (22.40 im/s)\n",
            "[TRAIN] epoch 5/20  observation 1100/1600 batch loss: 0.6401 (avg 0.6357),  avg acc: 0.6679, avg f1: 0.5519, (22.69 im/s)\n",
            "[TRAIN] epoch 5/20  observation 1200/1600 batch loss: 0.5173 (avg 0.6355),  avg acc: 0.6681, avg f1: 0.5522, (22.41 im/s)\n",
            "[TRAIN] epoch 5/20  observation 1300/1600 batch loss: 0.7505 (avg 0.6366),  avg acc: 0.6665, avg f1: 0.5504, (22.54 im/s)\n",
            "[TRAIN] epoch 5/20  observation 1400/1600 batch loss: 0.4065 (avg 0.6364),  avg acc: 0.6668, avg f1: 0.5507, (22.74 im/s)\n",
            "[TRAIN] epoch 5/20  observation 1500/1600 batch loss: 0.6411 (avg 0.6368),  avg acc: 0.6662, avg f1: 0.5501, (22.39 im/s)\n",
            "Epoch 4 validation: acc: 0.6675, f1: 0.5522 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|██▌       | 5/20 [39:09<1:57:33, 470.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  5\n",
            "[TRAIN] epoch 6/20  observation 0/1600 batch loss: 0.6304 (avg 0.6304),  avg acc: 0.6667, avg f1: 0.5333, (18.56 im/s)\n",
            "[TRAIN] epoch 6/20  observation 100/1600 batch loss: 0.8816 (avg 0.6228),  avg acc: 0.6881, avg f1: 0.5757, (22.32 im/s)\n",
            "[TRAIN] epoch 6/20  observation 200/1600 batch loss: 0.6410 (avg 0.6272),  avg acc: 0.6816, avg f1: 0.5709, (22.26 im/s)\n",
            "[TRAIN] epoch 6/20  observation 300/1600 batch loss: 0.5201 (avg 0.6312),  avg acc: 0.6755, avg f1: 0.5638, (22.45 im/s)\n",
            "[TRAIN] epoch 6/20  observation 400/1600 batch loss: 0.6361 (avg 0.6373),  avg acc: 0.6667, avg f1: 0.5525, (22.82 im/s)\n",
            "[TRAIN] epoch 6/20  observation 500/1600 batch loss: 0.5270 (avg 0.6384),  avg acc: 0.6647, avg f1: 0.5486, (22.39 im/s)\n",
            "[TRAIN] epoch 6/20  observation 600/1600 batch loss: 0.4016 (avg 0.6364),  avg acc: 0.6675, avg f1: 0.5516, (22.77 im/s)\n",
            "[TRAIN] epoch 6/20  observation 700/1600 batch loss: 0.6443 (avg 0.6361),  avg acc: 0.6679, avg f1: 0.5516, (22.81 im/s)\n",
            "[TRAIN] epoch 6/20  observation 800/1600 batch loss: 0.6352 (avg 0.6345),  avg acc: 0.6700, avg f1: 0.5546, (22.73 im/s)\n",
            "[TRAIN] epoch 6/20  observation 900/1600 batch loss: 0.5198 (avg 0.6347),  avg acc: 0.6698, avg f1: 0.5546, (22.35 im/s)\n",
            "[TRAIN] epoch 6/20  observation 1000/1600 batch loss: 0.5201 (avg 0.6346),  avg acc: 0.6698, avg f1: 0.5544, (22.70 im/s)\n",
            "[TRAIN] epoch 6/20  observation 1100/1600 batch loss: 0.7514 (avg 0.6348),  avg acc: 0.6697, avg f1: 0.5543, (22.52 im/s)\n",
            "[TRAIN] epoch 6/20  observation 1200/1600 batch loss: 0.5130 (avg 0.6342),  avg acc: 0.6704, avg f1: 0.5549, (22.47 im/s)\n",
            "[TRAIN] epoch 6/20  observation 1300/1600 batch loss: 0.8816 (avg 0.6340),  avg acc: 0.6706, avg f1: 0.5551, (22.33 im/s)\n",
            "[TRAIN] epoch 6/20  observation 1400/1600 batch loss: 0.7534 (avg 0.6356),  avg acc: 0.6683, avg f1: 0.5521, (22.45 im/s)\n",
            "[TRAIN] epoch 6/20  observation 1500/1600 batch loss: 0.9751 (avg 0.6363),  avg acc: 0.6673, avg f1: 0.5508, (22.49 im/s)\n",
            "Epoch 5 validation: acc: 0.6675, f1: 0.5522 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 6/20 [47:00<1:49:46, 470.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  6\n",
            "[TRAIN] epoch 7/20  observation 0/1600 batch loss: 0.6381 (avg 0.6381),  avg acc: 0.6667, avg f1: 0.5333, (16.84 im/s)\n",
            "[TRAIN] epoch 7/20  observation 100/1600 batch loss: 0.6316 (avg 0.6372),  avg acc: 0.6650, avg f1: 0.5532, (22.15 im/s)\n",
            "[TRAIN] epoch 7/20  observation 200/1600 batch loss: 0.6278 (avg 0.6381),  avg acc: 0.6642, avg f1: 0.5519, (22.57 im/s)\n",
            "[TRAIN] epoch 7/20  observation 300/1600 batch loss: 0.5284 (avg 0.6364),  avg acc: 0.6667, avg f1: 0.5519, (22.47 im/s)\n",
            "[TRAIN] epoch 7/20  observation 400/1600 batch loss: 0.6365 (avg 0.6344),  avg acc: 0.6696, avg f1: 0.5541, (22.31 im/s)\n",
            "[TRAIN] epoch 7/20  observation 500/1600 batch loss: 0.3899 (avg 0.6304),  avg acc: 0.6753, avg f1: 0.5605, (22.84 im/s)\n",
            "[TRAIN] epoch 7/20  observation 600/1600 batch loss: 0.7554 (avg 0.6333),  avg acc: 0.6714, avg f1: 0.5556, (22.28 im/s)\n",
            "[TRAIN] epoch 7/20  observation 700/1600 batch loss: 0.8680 (avg 0.6368),  avg acc: 0.6664, avg f1: 0.5494, (22.07 im/s)\n",
            "[TRAIN] epoch 7/20  observation 800/1600 batch loss: 0.6380 (avg 0.6343),  avg acc: 0.6700, avg f1: 0.5544, (22.56 im/s)\n",
            "[TRAIN] epoch 7/20  observation 900/1600 batch loss: 0.5236 (avg 0.6341),  avg acc: 0.6704, avg f1: 0.5547, (22.67 im/s)\n",
            "[TRAIN] epoch 7/20  observation 1000/1600 batch loss: 0.5223 (avg 0.6362),  avg acc: 0.6673, avg f1: 0.5513, (22.45 im/s)\n",
            "[TRAIN] epoch 7/20  observation 1100/1600 batch loss: 0.6336 (avg 0.6365),  avg acc: 0.6668, avg f1: 0.5509, (23.02 im/s)\n",
            "[TRAIN] epoch 7/20  observation 1200/1600 batch loss: 0.6369 (avg 0.6355),  avg acc: 0.6683, avg f1: 0.5527, (22.03 im/s)\n",
            "[TRAIN] epoch 7/20  observation 1300/1600 batch loss: 0.6307 (avg 0.6349),  avg acc: 0.6691, avg f1: 0.5535, (22.48 im/s)\n",
            "[TRAIN] epoch 7/20  observation 1400/1600 batch loss: 0.8718 (avg 0.6359),  avg acc: 0.6677, avg f1: 0.5518, (22.54 im/s)\n",
            "[TRAIN] epoch 7/20  observation 1500/1600 batch loss: 0.6379 (avg 0.6358),  avg acc: 0.6678, avg f1: 0.5520, (22.81 im/s)\n",
            "Epoch 6 validation: acc: 0.6675, f1: 0.5522 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 35%|███▌      | 7/20 [54:51<1:42:00, 470.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  7\n",
            "[TRAIN] epoch 8/20  observation 0/1600 batch loss: 0.5237 (avg 0.5237),  avg acc: 0.8333, avg f1: 0.7576, (16.54 im/s)\n",
            "[TRAIN] epoch 8/20  observation 100/1600 batch loss: 0.8766 (avg 0.6246),  avg acc: 0.6832, avg f1: 0.5702, (22.31 im/s)\n",
            "[TRAIN] epoch 8/20  observation 200/1600 batch loss: 0.6349 (avg 0.6302),  avg acc: 0.6758, avg f1: 0.5601, (22.65 im/s)\n",
            "[TRAIN] epoch 8/20  observation 300/1600 batch loss: 0.6336 (avg 0.6316),  avg acc: 0.6739, avg f1: 0.5583, (22.96 im/s)\n",
            "[TRAIN] epoch 8/20  observation 400/1600 batch loss: 0.7591 (avg 0.6289),  avg acc: 0.6775, avg f1: 0.5621, (22.52 im/s)\n",
            "[TRAIN] epoch 8/20  observation 500/1600 batch loss: 0.7622 (avg 0.6264),  avg acc: 0.6810, avg f1: 0.5669, (22.62 im/s)\n",
            "[TRAIN] epoch 8/20  observation 600/1600 batch loss: 0.7608 (avg 0.6261),  avg acc: 0.6814, avg f1: 0.5677, (21.99 im/s)\n",
            "[TRAIN] epoch 8/20  observation 700/1600 batch loss: 0.7689 (avg 0.6250),  avg acc: 0.6828, avg f1: 0.5695, (22.40 im/s)\n",
            "[TRAIN] epoch 8/20  observation 800/1600 batch loss: 0.7639 (avg 0.6266),  avg acc: 0.6806, avg f1: 0.5669, (22.41 im/s)\n",
            "[TRAIN] epoch 8/20  observation 900/1600 batch loss: 0.3988 (avg 0.6293),  avg acc: 0.6768, avg f1: 0.5621, (22.66 im/s)\n",
            "[TRAIN] epoch 8/20  observation 1000/1600 batch loss: 0.7552 (avg 0.6312),  avg acc: 0.6742, avg f1: 0.5591, (22.63 im/s)\n",
            "[TRAIN] epoch 8/20  observation 1100/1600 batch loss: 0.6425 (avg 0.6343),  avg acc: 0.6695, avg f1: 0.5533, (22.60 im/s)\n",
            "[TRAIN] epoch 8/20  observation 1200/1600 batch loss: 0.5343 (avg 0.6357),  avg acc: 0.6675, avg f1: 0.5511, (22.40 im/s)\n",
            "[TRAIN] epoch 8/20  observation 1300/1600 batch loss: 0.7454 (avg 0.6361),  avg acc: 0.6671, avg f1: 0.5506, (21.94 im/s)\n",
            "[TRAIN] epoch 8/20  observation 1400/1600 batch loss: 0.7406 (avg 0.6361),  avg acc: 0.6671, avg f1: 0.5506, (22.59 im/s)\n",
            "[TRAIN] epoch 8/20  observation 1500/1600 batch loss: 0.8639 (avg 0.6360),  avg acc: 0.6673, avg f1: 0.5509, (22.53 im/s)\n",
            "Epoch 7 validation: acc: 0.6675, f1: 0.5522 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 8/20 [1:02:39<1:33:58, 469.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  8\n",
            "[TRAIN] epoch 9/20  observation 0/1600 batch loss: 0.6375 (avg 0.6375),  avg acc: 0.6667, avg f1: 0.5333, (18.90 im/s)\n",
            "[TRAIN] epoch 9/20  observation 100/1600 batch loss: 0.7452 (avg 0.6403),  avg acc: 0.6617, avg f1: 0.5394, (22.01 im/s)\n",
            "[TRAIN] epoch 9/20  observation 200/1600 batch loss: 0.6427 (avg 0.6293),  avg acc: 0.6774, avg f1: 0.5624, (22.19 im/s)\n",
            "[TRAIN] epoch 9/20  observation 300/1600 batch loss: 0.7541 (avg 0.6314),  avg acc: 0.6744, avg f1: 0.5595, (22.39 im/s)\n",
            "[TRAIN] epoch 9/20  observation 400/1600 batch loss: 0.6346 (avg 0.6291),  avg acc: 0.6775, avg f1: 0.5635, (22.48 im/s)\n",
            "[TRAIN] epoch 9/20  observation 500/1600 batch loss: 0.6359 (avg 0.6314),  avg acc: 0.6743, avg f1: 0.5600, (22.64 im/s)\n",
            "[TRAIN] epoch 9/20  observation 600/1600 batch loss: 0.5166 (avg 0.6320),  avg acc: 0.6736, avg f1: 0.5590, (22.61 im/s)\n",
            "[TRAIN] epoch 9/20  observation 700/1600 batch loss: 0.7555 (avg 0.6342),  avg acc: 0.6705, avg f1: 0.5549, (22.45 im/s)\n",
            "[TRAIN] epoch 9/20  observation 800/1600 batch loss: 0.7515 (avg 0.6350),  avg acc: 0.6692, avg f1: 0.5530, (22.63 im/s)\n",
            "[TRAIN] epoch 9/20  observation 900/1600 batch loss: 0.7513 (avg 0.6364),  avg acc: 0.6672, avg f1: 0.5508, (22.34 im/s)\n",
            "[TRAIN] epoch 9/20  observation 1000/1600 batch loss: 0.7564 (avg 0.6350),  avg acc: 0.6692, avg f1: 0.5531, (22.76 im/s)\n",
            "[TRAIN] epoch 9/20  observation 1100/1600 batch loss: 0.7442 (avg 0.6374),  avg acc: 0.6656, avg f1: 0.5488, (22.47 im/s)\n",
            "[TRAIN] epoch 9/20  observation 1200/1600 batch loss: 0.3984 (avg 0.6351),  avg acc: 0.6692, avg f1: 0.5534, (22.37 im/s)\n",
            "[TRAIN] epoch 9/20  observation 1300/1600 batch loss: 0.5239 (avg 0.6357),  avg acc: 0.6683, avg f1: 0.5525, (22.51 im/s)\n",
            "[TRAIN] epoch 9/20  observation 1400/1600 batch loss: 0.4141 (avg 0.6365),  avg acc: 0.6671, avg f1: 0.5513, (22.41 im/s)\n",
            "[TRAIN] epoch 9/20  observation 1500/1600 batch loss: 0.6364 (avg 0.6366),  avg acc: 0.6671, avg f1: 0.5514, (22.24 im/s)\n",
            "Epoch 8 validation: acc: 0.6675, f1: 0.5522 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 45%|████▌     | 9/20 [1:10:30<1:26:13, 470.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  9\n",
            "[TRAIN] epoch 10/20  observation 0/1600 batch loss: 0.4085 (avg 0.4085),  avg acc: 1.0000, avg f1: 1.0000, (17.42 im/s)\n",
            "[TRAIN] epoch 10/20  observation 100/1600 batch loss: 0.5232 (avg 0.6492),  avg acc: 0.6485, avg f1: 0.5286, (22.10 im/s)\n",
            "[TRAIN] epoch 10/20  observation 200/1600 batch loss: 0.6379 (avg 0.6397),  avg acc: 0.6625, avg f1: 0.5465, (22.19 im/s)\n",
            "[TRAIN] epoch 10/20  observation 300/1600 batch loss: 0.4074 (avg 0.6393),  avg acc: 0.6633, avg f1: 0.5475, (22.74 im/s)\n",
            "[TRAIN] epoch 10/20  observation 400/1600 batch loss: 0.7616 (avg 0.6373),  avg acc: 0.6663, avg f1: 0.5497, (22.81 im/s)\n",
            "[TRAIN] epoch 10/20  observation 500/1600 batch loss: 0.5183 (avg 0.6380),  avg acc: 0.6650, avg f1: 0.5486, (22.60 im/s)\n",
            "[TRAIN] epoch 10/20  observation 600/1600 batch loss: 0.7553 (avg 0.6368),  avg acc: 0.6667, avg f1: 0.5503, (22.32 im/s)\n",
            "[TRAIN] epoch 10/20  observation 700/1600 batch loss: 0.7549 (avg 0.6348),  avg acc: 0.6695, avg f1: 0.5540, (22.48 im/s)\n",
            "[TRAIN] epoch 10/20  observation 800/1600 batch loss: 0.5208 (avg 0.6364),  avg acc: 0.6673, avg f1: 0.5513, (22.54 im/s)\n",
            "[TRAIN] epoch 10/20  observation 900/1600 batch loss: 0.5214 (avg 0.6365),  avg acc: 0.6672, avg f1: 0.5515, (22.32 im/s)\n",
            "[TRAIN] epoch 10/20  observation 1000/1600 batch loss: 0.6372 (avg 0.6371),  avg acc: 0.6663, avg f1: 0.5502, (22.41 im/s)\n",
            "[TRAIN] epoch 10/20  observation 1100/1600 batch loss: 0.7533 (avg 0.6365),  avg acc: 0.6671, avg f1: 0.5511, (22.29 im/s)\n",
            "[TRAIN] epoch 10/20  observation 1200/1600 batch loss: 0.5189 (avg 0.6370),  avg acc: 0.6664, avg f1: 0.5504, (22.71 im/s)\n",
            "[TRAIN] epoch 10/20  observation 1300/1600 batch loss: 0.6394 (avg 0.6375),  avg acc: 0.6656, avg f1: 0.5495, (22.40 im/s)\n",
            "[TRAIN] epoch 10/20  observation 1400/1600 batch loss: 0.7462 (avg 0.6376),  avg acc: 0.6656, avg f1: 0.5493, (22.55 im/s)\n",
            "[TRAIN] epoch 10/20  observation 1500/1600 batch loss: 0.5199 (avg 0.6373),  avg acc: 0.6660, avg f1: 0.5496, (22.50 im/s)\n",
            "Epoch 9 validation: acc: 0.6675, f1: 0.5522 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 10/20 [1:18:23<1:18:30, 471.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  10\n",
            "[TRAIN] epoch 11/20  observation 0/1600 batch loss: 0.6356 (avg 0.6356),  avg acc: 0.6667, avg f1: 0.5333, (18.94 im/s)\n",
            "[TRAIN] epoch 11/20  observation 100/1600 batch loss: 0.5092 (avg 0.6176),  avg acc: 0.6931, avg f1: 0.5817, (21.77 im/s)\n",
            "[TRAIN] epoch 11/20  observation 200/1600 batch loss: 0.5230 (avg 0.6297),  avg acc: 0.6766, avg f1: 0.5605, (22.06 im/s)\n",
            "[TRAIN] epoch 11/20  observation 300/1600 batch loss: 0.5132 (avg 0.6305),  avg acc: 0.6755, avg f1: 0.5590, (22.50 im/s)\n",
            "[TRAIN] epoch 11/20  observation 400/1600 batch loss: 0.7425 (avg 0.6356),  avg acc: 0.6683, avg f1: 0.5500, (22.59 im/s)\n",
            "[TRAIN] epoch 11/20  observation 500/1600 batch loss: 0.5241 (avg 0.6371),  avg acc: 0.6663, avg f1: 0.5477, (22.47 im/s)\n",
            "[TRAIN] epoch 11/20  observation 600/1600 batch loss: 0.6340 (avg 0.6354),  avg acc: 0.6689, avg f1: 0.5513, (22.57 im/s)\n",
            "[TRAIN] epoch 11/20  observation 700/1600 batch loss: 0.7455 (avg 0.6375),  avg acc: 0.6657, avg f1: 0.5480, (22.42 im/s)\n",
            "[TRAIN] epoch 11/20  observation 800/1600 batch loss: 0.7450 (avg 0.6385),  avg acc: 0.6644, avg f1: 0.5466, (22.59 im/s)\n",
            "[TRAIN] epoch 11/20  observation 900/1600 batch loss: 0.5247 (avg 0.6384),  avg acc: 0.6646, avg f1: 0.5472, (22.47 im/s)\n",
            "[TRAIN] epoch 11/20  observation 1000/1600 batch loss: 0.6384 (avg 0.6386),  avg acc: 0.6643, avg f1: 0.5466, (22.23 im/s)\n",
            "[TRAIN] epoch 11/20  observation 1100/1600 batch loss: 0.6384 (avg 0.6387),  avg acc: 0.6641, avg f1: 0.5462, (22.73 im/s)\n",
            "[TRAIN] epoch 11/20  observation 1200/1600 batch loss: 0.6332 (avg 0.6374),  avg acc: 0.6660, avg f1: 0.5484, (22.68 im/s)\n",
            "[TRAIN] epoch 11/20  observation 1300/1600 batch loss: 0.6397 (avg 0.6382),  avg acc: 0.6647, avg f1: 0.5468, (22.43 im/s)\n",
            "[TRAIN] epoch 11/20  observation 1400/1600 batch loss: 0.7546 (avg 0.6374),  avg acc: 0.6658, avg f1: 0.5483, (22.43 im/s)\n",
            "[TRAIN] epoch 11/20  observation 1500/1600 batch loss: 0.6376 (avg 0.6364),  avg acc: 0.6673, avg f1: 0.5499, (22.49 im/s)\n",
            "Epoch 10 validation: acc: 0.6675, f1: 0.5522 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 55%|█████▌    | 11/20 [1:26:19<1:10:53, 472.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  11\n",
            "[TRAIN] epoch 12/20  observation 0/1600 batch loss: 0.6401 (avg 0.6401),  avg acc: 0.6667, avg f1: 0.5333, (17.49 im/s)\n",
            "[TRAIN] epoch 12/20  observation 100/1600 batch loss: 1.0164 (avg 0.6323),  avg acc: 0.6733, avg f1: 0.5619, (21.91 im/s)\n",
            "[TRAIN] epoch 12/20  observation 200/1600 batch loss: 0.7513 (avg 0.6382),  avg acc: 0.6650, avg f1: 0.5522, (22.99 im/s)\n",
            "[TRAIN] epoch 12/20  observation 300/1600 batch loss: 0.6421 (avg 0.6350),  avg acc: 0.6694, avg f1: 0.5564, (22.78 im/s)\n",
            "[TRAIN] epoch 12/20  observation 400/1600 batch loss: 0.5177 (avg 0.6364),  avg acc: 0.6675, avg f1: 0.5530, (22.82 im/s)\n",
            "[TRAIN] epoch 12/20  observation 500/1600 batch loss: 0.5279 (avg 0.6395),  avg acc: 0.6627, avg f1: 0.5461, (22.31 im/s)\n",
            "[TRAIN] epoch 12/20  observation 600/1600 batch loss: 0.5282 (avg 0.6405),  avg acc: 0.6611, avg f1: 0.5436, (22.23 im/s)\n",
            "[TRAIN] epoch 12/20  observation 700/1600 batch loss: 0.6389 (avg 0.6383),  avg acc: 0.6643, avg f1: 0.5471, (22.79 im/s)\n",
            "[TRAIN] epoch 12/20  observation 800/1600 batch loss: 0.5218 (avg 0.6390),  avg acc: 0.6633, avg f1: 0.5461, (22.58 im/s)\n",
            "[TRAIN] epoch 12/20  observation 900/1600 batch loss: 0.6362 (avg 0.6418),  avg acc: 0.6589, avg f1: 0.5401, (22.67 im/s)\n",
            "[TRAIN] epoch 12/20  observation 1000/1600 batch loss: 0.7522 (avg 0.6410),  avg acc: 0.6603, avg f1: 0.5424, (22.60 im/s)\n",
            "[TRAIN] epoch 12/20  observation 1100/1600 batch loss: 0.7496 (avg 0.6408),  avg acc: 0.6606, avg f1: 0.5427, (22.72 im/s)\n",
            "[TRAIN] epoch 12/20  observation 1200/1600 batch loss: 0.6359 (avg 0.6385),  avg acc: 0.6639, avg f1: 0.5468, (22.50 im/s)\n",
            "[TRAIN] epoch 12/20  observation 1300/1600 batch loss: 0.5154 (avg 0.6378),  avg acc: 0.6650, avg f1: 0.5480, (22.50 im/s)\n",
            "[TRAIN] epoch 12/20  observation 1400/1600 batch loss: 0.6356 (avg 0.6381),  avg acc: 0.6645, avg f1: 0.5471, (22.87 im/s)\n",
            "[TRAIN] epoch 12/20  observation 1500/1600 batch loss: 0.6331 (avg 0.6375),  avg acc: 0.6656, avg f1: 0.5482, (22.34 im/s)\n",
            "Epoch 11 validation: acc: 0.6675, f1: 0.5522 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 12/20 [1:34:13<1:03:05, 473.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  12\n",
            "[TRAIN] epoch 13/20  observation 0/1600 batch loss: 0.6347 (avg 0.6347),  avg acc: 0.6667, avg f1: 0.5333, (16.81 im/s)\n",
            "[TRAIN] epoch 13/20  observation 100/1600 batch loss: 0.5134 (avg 0.6275),  avg acc: 0.6799, avg f1: 0.5645, (22.31 im/s)\n",
            "[TRAIN] epoch 13/20  observation 200/1600 batch loss: 0.7484 (avg 0.6431),  avg acc: 0.6584, avg f1: 0.5410, (22.46 im/s)\n",
            "[TRAIN] epoch 13/20  observation 300/1600 batch loss: 0.6370 (avg 0.6402),  avg acc: 0.6622, avg f1: 0.5455, (22.89 im/s)\n",
            "[TRAIN] epoch 13/20  observation 400/1600 batch loss: 0.5284 (avg 0.6419),  avg acc: 0.6596, avg f1: 0.5430, (22.58 im/s)\n",
            "[TRAIN] epoch 13/20  observation 500/1600 batch loss: 0.8606 (avg 0.6415),  avg acc: 0.6600, avg f1: 0.5433, (22.54 im/s)\n",
            "[TRAIN] epoch 13/20  observation 600/1600 batch loss: 0.6344 (avg 0.6421),  avg acc: 0.6592, avg f1: 0.5417, (22.38 im/s)\n",
            "[TRAIN] epoch 13/20  observation 700/1600 batch loss: 0.6420 (avg 0.6429),  avg acc: 0.6581, avg f1: 0.5404, (22.57 im/s)\n",
            "[TRAIN] epoch 13/20  observation 800/1600 batch loss: 0.7431 (avg 0.6425),  avg acc: 0.6588, avg f1: 0.5409, (22.22 im/s)\n",
            "[TRAIN] epoch 13/20  observation 900/1600 batch loss: 0.6386 (avg 0.6394),  avg acc: 0.6632, avg f1: 0.5465, (22.51 im/s)\n",
            "[TRAIN] epoch 13/20  observation 1000/1600 batch loss: 0.8798 (avg 0.6393),  avg acc: 0.6633, avg f1: 0.5467, (22.66 im/s)\n",
            "[TRAIN] epoch 13/20  observation 1100/1600 batch loss: 1.0359 (avg 0.6365),  avg acc: 0.6671, avg f1: 0.5513, (22.53 im/s)\n",
            "[TRAIN] epoch 13/20  observation 1200/1600 batch loss: 0.7751 (avg 0.6346),  avg acc: 0.6696, avg f1: 0.5542, (22.61 im/s)\n",
            "[TRAIN] epoch 13/20  observation 1300/1600 batch loss: 0.6309 (avg 0.6353),  avg acc: 0.6688, avg f1: 0.5532, (22.59 im/s)\n",
            "[TRAIN] epoch 13/20  observation 1400/1600 batch loss: 0.5179 (avg 0.6354),  avg acc: 0.6687, avg f1: 0.5531, (22.30 im/s)\n",
            "[TRAIN] epoch 13/20  observation 1500/1600 batch loss: 0.5107 (avg 0.6345),  avg acc: 0.6699, avg f1: 0.5544, (22.50 im/s)\n",
            "Epoch 12 validation: acc: 0.6675, f1: 0.5522 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 65%|██████▌   | 13/20 [1:42:01<55:00, 471.51s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  13\n",
            "[TRAIN] epoch 14/20  observation 0/1600 batch loss: 0.5267 (avg 0.5267),  avg acc: 0.8333, avg f1: 0.7576, (18.69 im/s)\n",
            "[TRAIN] epoch 14/20  observation 100/1600 batch loss: 0.6348 (avg 0.6136),  avg acc: 0.6997, avg f1: 0.5891, (22.21 im/s)\n",
            "[TRAIN] epoch 14/20  observation 200/1600 batch loss: 0.6385 (avg 0.6248),  avg acc: 0.6841, avg f1: 0.5714, (22.05 im/s)\n",
            "[TRAIN] epoch 14/20  observation 300/1600 batch loss: 0.6353 (avg 0.6282),  avg acc: 0.6794, avg f1: 0.5647, (22.66 im/s)\n",
            "[TRAIN] epoch 14/20  observation 400/1600 batch loss: 0.5196 (avg 0.6330),  avg acc: 0.6725, avg f1: 0.5563, (22.45 im/s)\n",
            "[TRAIN] epoch 14/20  observation 500/1600 batch loss: 0.8639 (avg 0.6343),  avg acc: 0.6707, avg f1: 0.5543, (22.41 im/s)\n",
            "[TRAIN] epoch 14/20  observation 600/1600 batch loss: 0.7398 (avg 0.6380),  avg acc: 0.6650, avg f1: 0.5477, (22.69 im/s)\n",
            "[TRAIN] epoch 14/20  observation 700/1600 batch loss: 0.7536 (avg 0.6370),  avg acc: 0.6667, avg f1: 0.5502, (22.31 im/s)\n",
            "[TRAIN] epoch 14/20  observation 800/1600 batch loss: 0.4262 (avg 0.6380),  avg acc: 0.6652, avg f1: 0.5481, (22.53 im/s)\n",
            "[TRAIN] epoch 14/20  observation 900/1600 batch loss: 0.8714 (avg 0.6372),  avg acc: 0.6663, avg f1: 0.5496, (22.83 im/s)\n",
            "[TRAIN] epoch 14/20  observation 1000/1600 batch loss: 0.8662 (avg 0.6379),  avg acc: 0.6653, avg f1: 0.5482, (22.39 im/s)\n",
            "[TRAIN] epoch 14/20  observation 1100/1600 batch loss: 0.6350 (avg 0.6378),  avg acc: 0.6655, avg f1: 0.5488, (22.59 im/s)\n",
            "[TRAIN] epoch 14/20  observation 1200/1600 batch loss: 0.4078 (avg 0.6377),  avg acc: 0.6656, avg f1: 0.5489, (22.55 im/s)\n",
            "[TRAIN] epoch 14/20  observation 1300/1600 batch loss: 0.7554 (avg 0.6366),  avg acc: 0.6671, avg f1: 0.5507, (22.37 im/s)\n",
            "[TRAIN] epoch 14/20  observation 1400/1600 batch loss: 0.5115 (avg 0.6359),  avg acc: 0.6681, avg f1: 0.5518, (22.59 im/s)\n",
            "[TRAIN] epoch 14/20  observation 1500/1600 batch loss: 0.6396 (avg 0.6358),  avg acc: 0.6682, avg f1: 0.5522, (22.67 im/s)\n",
            "Epoch 13 validation: acc: 0.6675, f1: 0.5522 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 14/20 [1:50:02<47:26, 474.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  14\n",
            "[TRAIN] epoch 15/20  observation 0/1600 batch loss: 0.7576 (avg 0.7576),  avg acc: 0.5000, avg f1: 0.3333, (18.74 im/s)\n",
            "[TRAIN] epoch 15/20  observation 100/1600 batch loss: 0.3995 (avg 0.6264),  avg acc: 0.6815, avg f1: 0.5687, (22.05 im/s)\n",
            "[TRAIN] epoch 15/20  observation 200/1600 batch loss: 0.6297 (avg 0.6319),  avg acc: 0.6741, avg f1: 0.5618, (22.54 im/s)\n",
            "[TRAIN] epoch 15/20  observation 300/1600 batch loss: 0.4066 (avg 0.6337),  avg acc: 0.6717, avg f1: 0.5580, (22.83 im/s)\n",
            "[TRAIN] epoch 15/20  observation 400/1600 batch loss: 0.7540 (avg 0.6370),  avg acc: 0.6667, avg f1: 0.5504, (22.32 im/s)\n",
            "[TRAIN] epoch 15/20  observation 500/1600 batch loss: 0.5210 (avg 0.6359),  avg acc: 0.6683, avg f1: 0.5526, (22.34 im/s)\n",
            "[TRAIN] epoch 15/20  observation 600/1600 batch loss: 0.7537 (avg 0.6357),  avg acc: 0.6686, avg f1: 0.5528, (22.48 im/s)\n",
            "[TRAIN] epoch 15/20  observation 700/1600 batch loss: 0.6352 (avg 0.6355),  avg acc: 0.6688, avg f1: 0.5529, (22.57 im/s)\n",
            "[TRAIN] epoch 15/20  observation 800/1600 batch loss: 0.8708 (avg 0.6354),  avg acc: 0.6690, avg f1: 0.5531, (22.71 im/s)\n",
            "[TRAIN] epoch 15/20  observation 900/1600 batch loss: 0.6363 (avg 0.6374),  avg acc: 0.6661, avg f1: 0.5503, (22.73 im/s)\n",
            "[TRAIN] epoch 15/20  observation 1000/1600 batch loss: 0.8685 (avg 0.6370),  avg acc: 0.6667, avg f1: 0.5510, (22.50 im/s)\n",
            "[TRAIN] epoch 15/20  observation 1100/1600 batch loss: 0.5251 (avg 0.6375),  avg acc: 0.6659, avg f1: 0.5501, (22.60 im/s)\n",
            "[TRAIN] epoch 15/20  observation 1200/1600 batch loss: 0.7450 (avg 0.6376),  avg acc: 0.6657, avg f1: 0.5501, (22.89 im/s)\n",
            "[TRAIN] epoch 15/20  observation 1300/1600 batch loss: 0.6364 (avg 0.6384),  avg acc: 0.6646, avg f1: 0.5487, (22.72 im/s)\n",
            "[TRAIN] epoch 15/20  observation 1400/1600 batch loss: 0.7661 (avg 0.6368),  avg acc: 0.6668, avg f1: 0.5511, (22.81 im/s)\n",
            "[TRAIN] epoch 15/20  observation 1500/1600 batch loss: 0.7653 (avg 0.6360),  avg acc: 0.6680, avg f1: 0.5528, (22.55 im/s)\n",
            "Epoch 14 validation: acc: 0.6675, f1: 0.5522 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|███████▌  | 15/20 [1:57:49<39:20, 472.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  15\n",
            "[TRAIN] epoch 16/20  observation 0/1600 batch loss: 0.7547 (avg 0.7547),  avg acc: 0.5000, avg f1: 0.3333, (18.08 im/s)\n",
            "[TRAIN] epoch 16/20  observation 100/1600 batch loss: 0.5404 (avg 0.6637),  avg acc: 0.6271, avg f1: 0.5021, (22.42 im/s)\n",
            "[TRAIN] epoch 16/20  observation 200/1600 batch loss: 0.6399 (avg 0.6606),  avg acc: 0.6310, avg f1: 0.5054, (22.53 im/s)\n",
            "[TRAIN] epoch 16/20  observation 300/1600 batch loss: 0.5339 (avg 0.6557),  avg acc: 0.6384, avg f1: 0.5138, (22.59 im/s)\n",
            "[TRAIN] epoch 16/20  observation 400/1600 batch loss: 0.5249 (avg 0.6498),  avg acc: 0.6475, avg f1: 0.5263, (22.72 im/s)\n",
            "[TRAIN] epoch 16/20  observation 500/1600 batch loss: 0.7674 (avg 0.6435),  avg acc: 0.6567, avg f1: 0.5375, (22.21 im/s)\n",
            "[TRAIN] epoch 16/20  observation 600/1600 batch loss: 0.5001 (avg 0.6373),  avg acc: 0.6650, avg f1: 0.5479, (22.37 im/s)\n",
            "[TRAIN] epoch 16/20  observation 700/1600 batch loss: 0.7666 (avg 0.6367),  avg acc: 0.6662, avg f1: 0.5498, (22.74 im/s)\n",
            "[TRAIN] epoch 16/20  observation 800/1600 batch loss: 0.6398 (avg 0.6392),  avg acc: 0.6629, avg f1: 0.5461, (22.45 im/s)\n",
            "[TRAIN] epoch 16/20  observation 900/1600 batch loss: 0.7447 (avg 0.6395),  avg acc: 0.6624, avg f1: 0.5451, (22.82 im/s)\n",
            "[TRAIN] epoch 16/20  observation 1000/1600 batch loss: 0.5186 (avg 0.6372),  avg acc: 0.6657, avg f1: 0.5492, (22.39 im/s)\n",
            "[TRAIN] epoch 16/20  observation 1100/1600 batch loss: 0.6380 (avg 0.6383),  avg acc: 0.6642, avg f1: 0.5476, (22.49 im/s)\n",
            "[TRAIN] epoch 16/20  observation 1200/1600 batch loss: 0.6308 (avg 0.6380),  avg acc: 0.6647, avg f1: 0.5483, (22.55 im/s)\n",
            "[TRAIN] epoch 16/20  observation 1300/1600 batch loss: 0.6382 (avg 0.6360),  avg acc: 0.6674, avg f1: 0.5515, (22.81 im/s)\n",
            "[TRAIN] epoch 16/20  observation 1400/1600 batch loss: 0.7578 (avg 0.6367),  avg acc: 0.6667, avg f1: 0.5505, (23.08 im/s)\n",
            "[TRAIN] epoch 16/20  observation 1500/1600 batch loss: 0.7567 (avg 0.6363),  avg acc: 0.6672, avg f1: 0.5509, (22.22 im/s)\n",
            "Epoch 15 validation: acc: 0.6675, f1: 0.5522 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 16/20 [2:05:39<31:25, 471.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  16\n",
            "[TRAIN] epoch 17/20  observation 0/1600 batch loss: 0.9953 (avg 0.9953),  avg acc: 0.1667, avg f1: 0.0476, (17.83 im/s)\n",
            "[TRAIN] epoch 17/20  observation 100/1600 batch loss: 0.9875 (avg 0.6417),  avg acc: 0.6601, avg f1: 0.5402, (22.44 im/s)\n",
            "[TRAIN] epoch 17/20  observation 200/1600 batch loss: 0.8712 (avg 0.6359),  avg acc: 0.6683, avg f1: 0.5503, (22.55 im/s)\n",
            "[TRAIN] epoch 17/20  observation 300/1600 batch loss: 0.7583 (avg 0.6339),  avg acc: 0.6711, avg f1: 0.5568, (22.54 im/s)\n",
            "[TRAIN] epoch 17/20  observation 400/1600 batch loss: 0.5326 (avg 0.6395),  avg acc: 0.6629, avg f1: 0.5465, (22.55 im/s)\n",
            "[TRAIN] epoch 17/20  observation 500/1600 batch loss: 0.3995 (avg 0.6376),  avg acc: 0.6657, avg f1: 0.5500, (22.56 im/s)\n",
            "[TRAIN] epoch 17/20  observation 600/1600 batch loss: 0.6415 (avg 0.6367),  avg acc: 0.6672, avg f1: 0.5515, (22.65 im/s)\n",
            "[TRAIN] epoch 17/20  observation 700/1600 batch loss: 0.5193 (avg 0.6355),  avg acc: 0.6688, avg f1: 0.5532, (22.03 im/s)\n",
            "[TRAIN] epoch 17/20  observation 800/1600 batch loss: 0.5237 (avg 0.6365),  avg acc: 0.6675, avg f1: 0.5520, (22.26 im/s)\n",
            "[TRAIN] epoch 17/20  observation 900/1600 batch loss: 0.5154 (avg 0.6341),  avg acc: 0.6707, avg f1: 0.5558, (22.26 im/s)\n",
            "[TRAIN] epoch 17/20  observation 1000/1600 batch loss: 0.5091 (avg 0.6331),  avg acc: 0.6722, avg f1: 0.5574, (22.56 im/s)\n",
            "[TRAIN] epoch 17/20  observation 1100/1600 batch loss: 0.7562 (avg 0.6354),  avg acc: 0.6689, avg f1: 0.5533, (21.91 im/s)\n",
            "[TRAIN] epoch 17/20  observation 1200/1600 batch loss: 0.6369 (avg 0.6353),  avg acc: 0.6692, avg f1: 0.5536, (22.54 im/s)\n",
            "[TRAIN] epoch 17/20  observation 1300/1600 batch loss: 0.7557 (avg 0.6344),  avg acc: 0.6704, avg f1: 0.5554, (22.46 im/s)\n",
            "[TRAIN] epoch 17/20  observation 1400/1600 batch loss: 0.5376 (avg 0.6369),  avg acc: 0.6667, avg f1: 0.5509, (22.27 im/s)\n",
            "[TRAIN] epoch 17/20  observation 1500/1600 batch loss: 0.6351 (avg 0.6355),  avg acc: 0.6689, avg f1: 0.5539, (22.42 im/s)\n",
            "Epoch 16 validation: acc: 0.6675, f1: 0.5522 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 85%|████████▌ | 17/20 [2:13:26<23:30, 470.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  17\n",
            "[TRAIN] epoch 18/20  observation 0/1600 batch loss: 0.6365 (avg 0.6365),  avg acc: 0.6667, avg f1: 0.5333, (16.93 im/s)\n",
            "[TRAIN] epoch 18/20  observation 100/1600 batch loss: 0.5056 (avg 0.6123),  avg acc: 0.7013, avg f1: 0.5939, (22.48 im/s)\n",
            "[TRAIN] epoch 18/20  observation 200/1600 batch loss: 0.5164 (avg 0.6244),  avg acc: 0.6841, avg f1: 0.5749, (22.63 im/s)\n",
            "[TRAIN] epoch 18/20  observation 300/1600 batch loss: 0.5258 (avg 0.6325),  avg acc: 0.6728, avg f1: 0.5605, (22.49 im/s)\n",
            "[TRAIN] epoch 18/20  observation 400/1600 batch loss: 0.5134 (avg 0.6313),  avg acc: 0.6746, avg f1: 0.5619, (22.67 im/s)\n",
            "[TRAIN] epoch 18/20  observation 500/1600 batch loss: 0.7522 (avg 0.6307),  avg acc: 0.6756, avg f1: 0.5636, (22.19 im/s)\n",
            "[TRAIN] epoch 18/20  observation 600/1600 batch loss: 0.5094 (avg 0.6292),  avg acc: 0.6775, avg f1: 0.5652, (22.29 im/s)\n",
            "[TRAIN] epoch 18/20  observation 700/1600 batch loss: 0.5234 (avg 0.6324),  avg acc: 0.6731, avg f1: 0.5600, (22.62 im/s)\n",
            "[TRAIN] epoch 18/20  observation 800/1600 batch loss: 0.7472 (avg 0.6343),  avg acc: 0.6704, avg f1: 0.5565, (22.68 im/s)\n",
            "[TRAIN] epoch 18/20  observation 900/1600 batch loss: 0.6383 (avg 0.6333),  avg acc: 0.6718, avg f1: 0.5578, (22.63 im/s)\n",
            "[TRAIN] epoch 18/20  observation 1000/1600 batch loss: 0.7581 (avg 0.6331),  avg acc: 0.6722, avg f1: 0.5586, (22.59 im/s)\n",
            "[TRAIN] epoch 18/20  observation 1100/1600 batch loss: 0.6347 (avg 0.6346),  avg acc: 0.6700, avg f1: 0.5557, (22.59 im/s)\n",
            "[TRAIN] epoch 18/20  observation 1200/1600 batch loss: 0.8758 (avg 0.6347),  avg acc: 0.6699, avg f1: 0.5556, (22.71 im/s)\n",
            "[TRAIN] epoch 18/20  observation 1300/1600 batch loss: 0.7361 (avg 0.6353),  avg acc: 0.6690, avg f1: 0.5544, (22.71 im/s)\n",
            "[TRAIN] epoch 18/20  observation 1400/1600 batch loss: 0.5180 (avg 0.6348),  avg acc: 0.6696, avg f1: 0.5551, (22.75 im/s)\n",
            "[TRAIN] epoch 18/20  observation 1500/1600 batch loss: 0.7356 (avg 0.6359),  avg acc: 0.6680, avg f1: 0.5528, (22.58 im/s)\n",
            "Epoch 17 validation: acc: 0.6675, f1: 0.5522 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 18/20 [2:21:15<15:39, 469.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  18\n",
            "[TRAIN] epoch 19/20  observation 0/1600 batch loss: 0.6424 (avg 0.6424),  avg acc: 0.6667, avg f1: 0.5333, (17.84 im/s)\n",
            "[TRAIN] epoch 19/20  observation 100/1600 batch loss: 0.7400 (avg 0.6520),  avg acc: 0.6436, avg f1: 0.5278, (22.50 im/s)\n",
            "[TRAIN] epoch 19/20  observation 200/1600 batch loss: 0.6401 (avg 0.6528),  avg acc: 0.6426, avg f1: 0.5227, (22.46 im/s)\n",
            "[TRAIN] epoch 19/20  observation 300/1600 batch loss: 0.6380 (avg 0.6567),  avg acc: 0.6357, avg f1: 0.5143, (22.81 im/s)\n",
            "[TRAIN] epoch 19/20  observation 400/1600 batch loss: 0.6428 (avg 0.6479),  avg acc: 0.6505, avg f1: 0.5328, (22.41 im/s)\n",
            "[TRAIN] epoch 19/20  observation 500/1600 batch loss: 0.8893 (avg 0.6429),  avg acc: 0.6577, avg f1: 0.5409, (22.35 im/s)\n",
            "[TRAIN] epoch 19/20  observation 600/1600 batch loss: 0.8920 (avg 0.6386),  avg acc: 0.6636, avg f1: 0.5479, (22.58 im/s)\n",
            "[TRAIN] epoch 19/20  observation 700/1600 batch loss: 1.0220 (avg 0.6376),  avg acc: 0.6652, avg f1: 0.5498, (22.30 im/s)\n",
            "[TRAIN] epoch 19/20  observation 800/1600 batch loss: 0.5122 (avg 0.6369),  avg acc: 0.6663, avg f1: 0.5508, (22.46 im/s)\n",
            "[TRAIN] epoch 19/20  observation 900/1600 batch loss: 0.6356 (avg 0.6361),  avg acc: 0.6674, avg f1: 0.5522, (22.49 im/s)\n",
            "[TRAIN] epoch 19/20  observation 1000/1600 batch loss: 0.3998 (avg 0.6358),  avg acc: 0.6680, avg f1: 0.5532, (22.50 im/s)\n",
            "[TRAIN] epoch 19/20  observation 1100/1600 batch loss: 0.8603 (avg 0.6368),  avg acc: 0.6665, avg f1: 0.5517, (22.88 im/s)\n",
            "[TRAIN] epoch 19/20  observation 1200/1600 batch loss: 0.8683 (avg 0.6364),  avg acc: 0.6672, avg f1: 0.5527, (22.57 im/s)\n",
            "[TRAIN] epoch 19/20  observation 1300/1600 batch loss: 0.4114 (avg 0.6372),  avg acc: 0.6662, avg f1: 0.5514, (22.44 im/s)\n",
            "[TRAIN] epoch 19/20  observation 1400/1600 batch loss: 0.7546 (avg 0.6379),  avg acc: 0.6651, avg f1: 0.5502, (22.76 im/s)\n",
            "[TRAIN] epoch 19/20  observation 1500/1600 batch loss: 0.6372 (avg 0.6386),  avg acc: 0.6641, avg f1: 0.5489, (22.68 im/s)\n",
            "Epoch 18 validation: acc: 0.6675, f1: 0.5522 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 95%|█████████▌| 19/20 [2:29:07<07:50, 470.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  19\n",
            "[TRAIN] epoch 20/20  observation 0/1600 batch loss: 0.3671 (avg 0.3671),  avg acc: 1.0000, avg f1: 1.0000, (16.63 im/s)\n",
            "[TRAIN] epoch 20/20  observation 100/1600 batch loss: 0.5215 (avg 0.6382),  avg acc: 0.6667, avg f1: 0.5519, (22.09 im/s)\n",
            "[TRAIN] epoch 20/20  observation 200/1600 batch loss: 0.3971 (avg 0.6380),  avg acc: 0.6667, avg f1: 0.5528, (22.27 im/s)\n",
            "[TRAIN] epoch 20/20  observation 300/1600 batch loss: 0.6380 (avg 0.6355),  avg acc: 0.6694, avg f1: 0.5560, (22.75 im/s)\n",
            "[TRAIN] epoch 20/20  observation 400/1600 batch loss: 0.4132 (avg 0.6387),  avg acc: 0.6646, avg f1: 0.5488, (22.41 im/s)\n",
            "[TRAIN] epoch 20/20  observation 500/1600 batch loss: 0.5188 (avg 0.6354),  avg acc: 0.6693, avg f1: 0.5539, (22.50 im/s)\n",
            "[TRAIN] epoch 20/20  observation 600/1600 batch loss: 0.3980 (avg 0.6346),  avg acc: 0.6705, avg f1: 0.5554, (22.29 im/s)\n",
            "[TRAIN] epoch 20/20  observation 700/1600 batch loss: 0.6353 (avg 0.6331),  avg acc: 0.6726, avg f1: 0.5584, (22.69 im/s)\n",
            "[TRAIN] epoch 20/20  observation 800/1600 batch loss: 0.6310 (avg 0.6346),  avg acc: 0.6704, avg f1: 0.5558, (22.56 im/s)\n",
            "[TRAIN] epoch 20/20  observation 900/1600 batch loss: 0.6361 (avg 0.6357),  avg acc: 0.6687, avg f1: 0.5537, (22.60 im/s)\n",
            "[TRAIN] epoch 20/20  observation 1000/1600 batch loss: 0.5196 (avg 0.6362),  avg acc: 0.6680, avg f1: 0.5530, (22.69 im/s)\n",
            "[TRAIN] epoch 20/20  observation 1100/1600 batch loss: 0.6371 (avg 0.6361),  avg acc: 0.6682, avg f1: 0.5533, (22.68 im/s)\n",
            "[TRAIN] epoch 20/20  observation 1200/1600 batch loss: 0.6341 (avg 0.6359),  avg acc: 0.6685, avg f1: 0.5538, (22.87 im/s)\n",
            "[TRAIN] epoch 20/20  observation 1300/1600 batch loss: 0.5257 (avg 0.6359),  avg acc: 0.6685, avg f1: 0.5535, (22.33 im/s)\n",
            "[TRAIN] epoch 20/20  observation 1400/1600 batch loss: 0.5336 (avg 0.6373),  avg acc: 0.6664, avg f1: 0.5509, (22.44 im/s)\n",
            "[TRAIN] epoch 20/20  observation 1500/1600 batch loss: 0.4024 (avg 0.6367),  avg acc: 0.6673, avg f1: 0.5521, (22.55 im/s)\n",
            "Epoch 19 validation: acc: 0.6675, f1: 0.5522 \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [2:36:56<00:00, 470.84s/it]\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 20\n",
        "BATCH_SIZE = 6\n",
        "ACCUMULATION_STEPS = 5\n",
        "FOLD = 4\n",
        "LR = 0.0001\n",
        "LR_DC_STEP = 80\n",
        "LR_DC = 0.1\n",
        "CUR_DIR = os.path.dirname(os.getcwd())\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "FOLD = 4\n",
        "CKPT_PATH2 = '/content/model_ckpt2'\n",
        "\n",
        "if not os.path.exists(CKPT_PATH2):\n",
        "    os.mkdir(CKPT_PATH2)\n",
        "\n",
        "# Khởi tạo DataLoader\n",
        "splits = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=123).split(X, y))\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(splits):\n",
        "    best_score = 0\n",
        "    if fold != FOLD:\n",
        "        continue\n",
        "    print(\"Training for fold {}\".format(fold))\n",
        "\n",
        "    # Create dataset\n",
        "    train_dataset = torch.utils.data.TensorDataset(torch.tensor(X[train_idx],dtype=torch.long), torch.tensor(y[train_idx],dtype=torch.long))\n",
        "    valid_dataset = torch.utils.data.TensorDataset(torch.tensor(X[val_idx],dtype=torch.long), torch.tensor(y[val_idx],dtype=torch.long))\n",
        "\n",
        "    # Create DataLoader\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    # Khởi tạo model:\n",
        "    MODEL_LAST_CKPT = os.path.join(CKPT_PATH2, 'latest_checkpoint.pth.tar')\n",
        "    if os.path.exists(MODEL_LAST_CKPT):\n",
        "      print('Load checkpoint model!')\n",
        "      checkpoint = torch.load(MODEL_LAST_CKPT)\n",
        "      state_dict = checkpoint['state_dict']\n",
        "\n",
        "      # Xác định các khóa không mong đợi\n",
        "      unexpected_keys = []\n",
        "      for key in state_dict.keys():\n",
        "          if 'classification_heads.new_task' in key:\n",
        "              unexpected_keys.append(key)\n",
        "\n",
        "      # Loại bỏ các khóa không mong đợi\n",
        "      for key in unexpected_keys:\n",
        "          del state_dict[key]\n",
        "\n",
        "      # Tải lại state_dict vào phoBERT_cls_temp\n",
        "      phoBERT_cls.load_state_dict(state_dict)\n",
        "\n",
        "    else:\n",
        "      print('Load model pretrained!')\n",
        "      # Load the model in fairseq\n",
        "      from fairseq.models.roberta import RobertaModel\n",
        "      from fairseq.data.encoders.fastbpe import fastBPE\n",
        "      from fairseq.data import Dictionary\n",
        "\n",
        "      phoBERT_cls = RobertaModel.from_pretrained('PhoBERT_base_fairseq', checkpoint_file='model.pt')\n",
        "      phoBERT_cls.eval()  # disable dropout (or leave in train mode to finetune\n",
        "\n",
        "      # # Load BPE\n",
        "      # class BPE():\n",
        "      #   bpe_codes = 'PhoBERT_base_fairseq/bpe.codes'\n",
        "\n",
        "      # args = BPE()\n",
        "      # phoBERT_cls.bpe = fastBPE(args) #Incorporate the BPE encoder into PhoBERT\n",
        "\n",
        "      # Add header cho classification với số lượng classes = 10\n",
        "      phoBERT_cls.register_classification_head('new_task', num_classes=2)\n",
        "\n",
        "    ## Load BPE\n",
        "    print('Load BPE')\n",
        "    class BPE():\n",
        "      bpe_codes = 'PhoBERT_base_fairseq/bpe.codes'\n",
        "\n",
        "    args = BPE()\n",
        "    phoBERT_cls.bpe = fastBPE(args) #Incorporate the BPE encoder into PhoBERT\n",
        "    phoBERT_cls.to(DEVICE)\n",
        "\n",
        "    # Khởi tạo optimizer và scheduler, criteria\n",
        "    print('Init Optimizer, scheduler, criteria')\n",
        "    param_optimizer = list(phoBERT_cls.named_parameters())\n",
        "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "\n",
        "    num_train_optimization_steps = int(EPOCHS*len(train_dataset)/BATCH_SIZE/ACCUMULATION_STEPS)\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=LR, correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=num_train_optimization_steps)  # scheduler với linear warmup\n",
        "    scheduler0 = get_constant_schedule(optimizer)  # scheduler với hằng số\n",
        "    # optimizer = optim.Adam(phoBERT_cls.parameters(), LR)\n",
        "    criteria = nn.NLLLoss()\n",
        "    # scheduler = StepLR(optimizer, step_size = LR_DC_STEP, gamma = LR_DC)\n",
        "    avg_loss = 0.\n",
        "    avg_accuracy = 0.\n",
        "    frozen = True\n",
        "    for epoch in tqdm(range(EPOCHS)):\n",
        "        # warm up tại epoch đầu tiên, sau epoch đầu sẽ phá băng các layers\n",
        "        if epoch > 0 and frozen:\n",
        "            for child in phoBERT_cls.children():\n",
        "                for param in child.parameters():\n",
        "                    param.requires_grad = True\n",
        "            frozen = False\n",
        "            del scheduler0\n",
        "            torch.cuda.empty_cache()\n",
        "        # Train model on EPOCH\n",
        "        print('Epoch: ', epoch)\n",
        "        trainOnEpoch(train_loader=train_loader, model=phoBERT_cls, optimizer=optimizer, epoch=epoch, num_epochs=EPOCHS, criteria=criteria, device=DEVICE, log_aggr=100)\n",
        "        # scheduler.step(epoch = epoch)\n",
        "        # Phá băng layers sau epoch đầu tiên\n",
        "        if not frozen:\n",
        "            scheduler.step()\n",
        "        else:\n",
        "            scheduler0.step()\n",
        "        optimizer.zero_grad()\n",
        "        # Validate on validation set\n",
        "        acc, f1 = validate(valid_loader, phoBERT_cls, device=DEVICE)\n",
        "        print('Epoch {} validation: acc: {:.4f}, f1: {:.4f} \\n'.format(epoch, acc, f1))\n",
        "\n",
        "        # Store best model checkpoint\n",
        "        ckpt_dict = {\n",
        "            'epoch': epoch + 1,\n",
        "            'state_dict': phoBERT_cls.state_dict(),\n",
        "            'optimizer': optimizer.state_dict()\n",
        "        }\n",
        "        # Save model checkpoint into 'latest_checkpoint.pth.tar'\n",
        "        torch.save(ckpt_dict, MODEL_LAST_CKPT)\n",
        "        #for key in phoBERT_cls.state_dict():\n",
        "          #print(key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvX8T9b_39pZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aee697e4-bd7f-484d-b186-4e2f2fe8b131"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fairseq.models.roberta.model:re-registering head \"new_task\" with num_classes 2 (prev: 2) and inner_dim None (prev: 768)\n",
            "100%|██████████| 22/22 [00:00<00:00, 6226.78it/s]\n",
            "100%|██████████| 120/120 [00:00<00:00, 12344.66it/s]\n",
            "100%|██████████| 241/241 [00:00<00:00, 11136.63it/s]\n",
            "100%|██████████| 96/96 [00:00<00:00, 8243.32it/s]\n",
            "100%|██████████| 215/215 [00:00<00:00, 10479.55it/s]\n",
            "100%|██████████| 48/48 [00:00<00:00, 6108.02it/s]\n",
            "100%|██████████| 596/596 [00:00<00:00, 12604.72it/s]\n",
            "100%|██████████| 327/327 [00:00<00:00, 12633.91it/s]\n",
            "100%|██████████| 173/173 [00:00<00:00, 11482.88it/s]\n",
            "100%|██████████| 136/136 [00:00<00:00, 10835.32it/s]\n",
            "100%|██████████| 126/126 [00:00<00:00, 11200.69it/s]\n",
            "100%|██████████| 42/42 [00:00<00:00, 7413.86it/s]\n",
            "100%|██████████| 389/389 [00:00<00:00, 12495.67it/s]\n",
            "100%|██████████| 157/157 [00:00<00:00, 9876.80it/s]\n",
            "100%|██████████| 48/48 [00:00<00:00, 9356.19it/s]\n",
            "100%|██████████| 83/83 [00:00<00:00, 10145.93it/s]\n",
            "100%|██████████| 97/97 [00:00<00:00, 10805.18it/s]\n",
            "100%|██████████| 146/146 [00:00<00:00, 11958.45it/s]\n",
            "100%|██████████| 189/189 [00:00<00:00, 11727.72it/s]\n",
            "100%|██████████| 274/274 [00:00<00:00, 11131.29it/s]\n",
            "100%|██████████| 321/321 [00:00<00:00, 12560.14it/s]\n",
            "100%|██████████| 295/295 [00:00<00:00, 12782.62it/s]\n",
            "100%|██████████| 383/383 [00:00<00:00, 12812.50it/s]\n",
            "100%|██████████| 161/161 [00:00<00:00, 11317.72it/s]\n",
            "100%|██████████| 145/145 [00:00<00:00, 11538.55it/s]\n",
            "100%|██████████| 216/216 [00:00<00:00, 10759.10it/s]\n",
            "100%|██████████| 378/378 [00:00<00:00, 9973.37it/s]\n",
            "100%|██████████| 217/217 [00:00<00:00, 13306.49it/s]\n",
            "100%|██████████| 242/242 [00:00<00:00, 12260.05it/s]\n",
            "100%|██████████| 278/278 [00:00<00:00, 12366.67it/s]\n",
            "100%|██████████| 71/71 [00:00<00:00, 8801.67it/s]\n",
            "100%|██████████| 145/145 [00:00<00:00, 10558.76it/s]\n",
            "100%|██████████| 127/127 [00:00<00:00, 11838.31it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 11849.84it/s]\n",
            "100%|██████████| 129/129 [00:00<00:00, 10757.19it/s]\n",
            "100%|██████████| 213/213 [00:00<00:00, 12520.66it/s]\n",
            "100%|██████████| 273/273 [00:00<00:00, 12494.22it/s]\n",
            "100%|██████████| 70/70 [00:00<00:00, 8595.89it/s]\n",
            "100%|██████████| 69/69 [00:00<00:00, 9538.79it/s]\n",
            "100%|██████████| 91/91 [00:00<00:00, 10361.93it/s]\n",
            "100%|██████████| 130/130 [00:00<00:00, 10209.51it/s]\n",
            "100%|██████████| 128/128 [00:00<00:00, 10670.83it/s]\n",
            "100%|██████████| 63/63 [00:00<00:00, 7893.45it/s]\n",
            "100%|██████████| 63/63 [00:00<00:00, 9135.07it/s]\n",
            "100%|██████████| 69/69 [00:00<00:00, 9159.32it/s]\n",
            "100%|██████████| 94/94 [00:00<00:00, 8599.76it/s]\n",
            "100%|██████████| 140/140 [00:00<00:00, 9996.47it/s]\n",
            "100%|██████████| 173/173 [00:00<00:00, 12182.71it/s]\n",
            "100%|██████████| 35/35 [00:00<00:00, 5477.02it/s]\n",
            "100%|██████████| 156/156 [00:00<00:00, 10589.11it/s]\n",
            "100%|██████████| 148/148 [00:00<00:00, 9189.73it/s]\n",
            "100%|██████████| 39/39 [00:00<00:00, 9586.70it/s]\n",
            "100%|██████████| 315/315 [00:00<00:00, 8987.06it/s]\n",
            "100%|██████████| 151/151 [00:00<00:00, 10980.81it/s]\n",
            "100%|██████████| 66/66 [00:00<00:00, 9900.36it/s]\n",
            "100%|██████████| 73/73 [00:00<00:00, 9706.88it/s]\n",
            "100%|██████████| 60/60 [00:00<00:00, 9043.02it/s]\n",
            "100%|██████████| 30/30 [00:00<00:00, 6972.69it/s]\n",
            "100%|██████████| 294/294 [00:00<00:00, 12431.33it/s]\n",
            "100%|██████████| 185/185 [00:00<00:00, 11536.52it/s]\n",
            "100%|██████████| 139/139 [00:00<00:00, 11488.98it/s]\n",
            "100%|██████████| 143/143 [00:00<00:00, 10977.04it/s]\n",
            "100%|██████████| 174/174 [00:00<00:00, 10306.14it/s]\n",
            "100%|██████████| 80/80 [00:00<00:00, 9757.88it/s]\n",
            "100%|██████████| 44/44 [00:00<00:00, 8415.38it/s]\n",
            "100%|██████████| 114/114 [00:00<00:00, 9295.31it/s]\n",
            "100%|██████████| 236/236 [00:00<00:00, 12415.88it/s]\n",
            "100%|██████████| 179/179 [00:00<00:00, 10764.80it/s]\n",
            "100%|██████████| 63/63 [00:00<00:00, 7919.47it/s]\n",
            "100%|██████████| 131/131 [00:00<00:00, 10902.08it/s]\n",
            "100%|██████████| 101/101 [00:00<00:00, 9721.96it/s]\n",
            "100%|██████████| 120/120 [00:00<00:00, 9854.65it/s]\n",
            "100%|██████████| 95/95 [00:00<00:00, 10312.88it/s]\n",
            "100%|██████████| 149/149 [00:00<00:00, 11147.11it/s]\n",
            "100%|██████████| 37/37 [00:00<00:00, 10925.74it/s]\n",
            "100%|██████████| 104/104 [00:00<00:00, 5608.80it/s]\n",
            "100%|██████████| 189/189 [00:00<00:00, 10259.93it/s]\n",
            "100%|██████████| 87/87 [00:00<00:00, 9747.16it/s]\n",
            "100%|██████████| 63/63 [00:00<00:00, 8879.97it/s]\n",
            "100%|██████████| 141/141 [00:00<00:00, 10379.03it/s]\n",
            "100%|██████████| 124/124 [00:00<00:00, 10013.16it/s]\n",
            "100%|██████████| 559/559 [00:00<00:00, 12751.42it/s]\n",
            "100%|██████████| 149/149 [00:00<00:00, 12334.24it/s]\n",
            "100%|██████████| 62/62 [00:00<00:00, 8905.10it/s]\n",
            "100%|██████████| 264/264 [00:00<00:00, 12070.99it/s]\n",
            "100%|██████████| 88/88 [00:00<00:00, 10204.84it/s]\n",
            "100%|██████████| 41/41 [00:00<00:00, 10134.75it/s]\n",
            "100%|██████████| 210/210 [00:00<00:00, 11768.06it/s]\n",
            "100%|██████████| 87/87 [00:00<00:00, 9588.11it/s]\n",
            "100%|██████████| 113/113 [00:00<00:00, 9045.14it/s]\n",
            "100%|██████████| 43/43 [00:00<00:00, 6532.24it/s]\n",
            "100%|██████████| 201/201 [00:00<00:00, 11567.72it/s]\n",
            "100%|██████████| 111/111 [00:00<00:00, 9158.59it/s]\n",
            "100%|██████████| 269/269 [00:00<00:00, 12234.39it/s]\n",
            "100%|██████████| 33/33 [00:00<00:00, 5342.44it/s]\n",
            "100%|██████████| 81/81 [00:00<00:00, 9168.99it/s]\n",
            "100%|██████████| 18/18 [00:00<00:00, 4819.81it/s]\n",
            "100%|██████████| 62/62 [00:00<00:00, 8534.80it/s]\n",
            "100%|██████████| 175/175 [00:00<00:00, 10448.44it/s]\n",
            "100%|██████████| 144/144 [00:00<00:00, 9690.34it/s]\n",
            "100%|██████████| 140/140 [00:00<00:00, 9854.05it/s]\n",
            "100%|██████████| 121/121 [00:00<00:00, 10682.41it/s]\n",
            "100%|██████████| 63/63 [00:00<00:00, 9232.42it/s]\n",
            "100%|██████████| 194/194 [00:00<00:00, 12764.44it/s]\n",
            "100%|██████████| 178/178 [00:00<00:00, 10934.02it/s]\n",
            "100%|██████████| 240/240 [00:00<00:00, 10999.17it/s]\n",
            "100%|██████████| 173/173 [00:00<00:00, 7107.74it/s]\n",
            "100%|██████████| 105/105 [00:00<00:00, 8583.66it/s]\n",
            "100%|██████████| 128/128 [00:00<00:00, 10813.33it/s]\n",
            "100%|██████████| 93/93 [00:00<00:00, 9835.11it/s]\n",
            "100%|██████████| 157/157 [00:00<00:00, 10769.40it/s]\n",
            "100%|██████████| 182/182 [00:00<00:00, 11579.97it/s]\n",
            "100%|██████████| 179/179 [00:00<00:00, 12098.63it/s]\n",
            "100%|██████████| 169/169 [00:00<00:00, 11110.65it/s]\n",
            "100%|██████████| 136/136 [00:00<00:00, 7719.82it/s]\n",
            "100%|██████████| 140/140 [00:00<00:00, 9029.58it/s]\n",
            "100%|██████████| 91/91 [00:00<00:00, 9941.18it/s]\n",
            "100%|██████████| 51/51 [00:00<00:00, 8501.97it/s]\n",
            "100%|██████████| 92/92 [00:00<00:00, 9636.06it/s]\n",
            "100%|██████████| 91/91 [00:00<00:00, 8572.69it/s]\n",
            "100%|██████████| 118/118 [00:00<00:00, 9256.70it/s]\n",
            "100%|██████████| 54/54 [00:00<00:00, 8282.17it/s]\n",
            "100%|██████████| 178/178 [00:00<00:00, 11590.43it/s]\n",
            "100%|██████████| 275/275 [00:00<00:00, 12413.19it/s]\n",
            "100%|██████████| 44/44 [00:00<00:00, 8054.70it/s]\n",
            "100%|██████████| 90/90 [00:00<00:00, 9956.67it/s]\n",
            "100%|██████████| 124/124 [00:00<00:00, 10230.61it/s]\n",
            "100%|██████████| 260/260 [00:00<00:00, 12308.20it/s]\n",
            "100%|██████████| 387/387 [00:00<00:00, 12039.28it/s]\n",
            "100%|██████████| 69/69 [00:00<00:00, 12116.18it/s]\n",
            "100%|██████████| 125/125 [00:00<00:00, 10055.78it/s]\n",
            "100%|██████████| 109/109 [00:00<00:00, 9801.04it/s]\n",
            "100%|██████████| 717/717 [00:00<00:00, 13772.28it/s]\n",
            "100%|██████████| 268/268 [00:00<00:00, 12105.69it/s]\n",
            "100%|██████████| 34/34 [00:00<00:00, 5949.12it/s]\n",
            "100%|██████████| 212/212 [00:00<00:00, 12233.68it/s]\n",
            "100%|██████████| 48/48 [00:00<00:00, 6883.67it/s]\n",
            "100%|██████████| 298/298 [00:00<00:00, 13686.46it/s]\n",
            "100%|██████████| 256/256 [00:00<00:00, 11287.81it/s]\n",
            "100%|██████████| 145/145 [00:00<00:00, 11146.68it/s]\n",
            "100%|██████████| 208/208 [00:00<00:00, 11654.42it/s]\n",
            "100%|██████████| 228/228 [00:00<00:00, 11832.78it/s]\n",
            "100%|██████████| 109/109 [00:00<00:00, 10615.29it/s]\n",
            "100%|██████████| 48/48 [00:00<00:00, 6767.74it/s]\n",
            "100%|██████████| 33/33 [00:00<00:00, 6266.96it/s]\n",
            "100%|██████████| 49/49 [00:00<00:00, 7807.06it/s]\n",
            "100%|██████████| 195/195 [00:00<00:00, 11304.31it/s]\n",
            "100%|██████████| 345/345 [00:00<00:00, 12641.50it/s]\n",
            "100%|██████████| 104/104 [00:00<00:00, 9877.00it/s]\n",
            "100%|██████████| 135/135 [00:00<00:00, 11107.58it/s]\n",
            "100%|██████████| 234/234 [00:00<00:00, 12249.81it/s]\n",
            "100%|██████████| 234/234 [00:00<00:00, 11230.37it/s]\n",
            "100%|██████████| 221/221 [00:00<00:00, 11186.43it/s]\n",
            "100%|██████████| 239/239 [00:00<00:00, 9259.55it/s]\n",
            "100%|██████████| 25/25 [00:00<00:00, 6346.16it/s]\n",
            "100%|██████████| 299/299 [00:00<00:00, 12155.28it/s]\n",
            "100%|██████████| 125/125 [00:00<00:00, 8457.76it/s]\n",
            "100%|██████████| 140/140 [00:00<00:00, 9704.06it/s]\n",
            "100%|██████████| 24/24 [00:00<00:00, 6649.27it/s]\n",
            "100%|██████████| 207/207 [00:00<00:00, 9091.99it/s]\n",
            "100%|██████████| 172/172 [00:00<00:00, 6196.54it/s]\n",
            "100%|██████████| 136/136 [00:00<00:00, 6639.80it/s]\n",
            "100%|██████████| 149/149 [00:00<00:00, 8583.55it/s]\n",
            "100%|██████████| 156/156 [00:00<00:00, 7799.17it/s]\n",
            "100%|██████████| 229/229 [00:00<00:00, 7135.29it/s]\n",
            "100%|██████████| 41/41 [00:00<00:00, 6441.66it/s]\n",
            "100%|██████████| 80/80 [00:00<00:00, 7474.48it/s]\n",
            "100%|██████████| 56/56 [00:00<00:00, 7648.85it/s]\n",
            "100%|██████████| 76/76 [00:00<00:00, 6455.13it/s]\n",
            "100%|██████████| 52/52 [00:00<00:00, 6581.48it/s]\n",
            "100%|██████████| 158/158 [00:00<00:00, 6909.03it/s]\n",
            "100%|██████████| 93/93 [00:00<00:00, 6885.01it/s]\n",
            "100%|██████████| 112/112 [00:00<00:00, 6054.81it/s]\n",
            "100%|██████████| 86/86 [00:00<00:00, 6101.73it/s]\n",
            "100%|██████████| 99/99 [00:00<00:00, 7632.60it/s]\n",
            "100%|██████████| 293/293 [00:00<00:00, 7816.33it/s]\n",
            "100%|██████████| 140/140 [00:00<00:00, 5555.95it/s]\n",
            "100%|██████████| 121/121 [00:00<00:00, 7582.26it/s]\n",
            "100%|██████████| 37/37 [00:00<00:00, 6965.72it/s]\n",
            "100%|██████████| 112/112 [00:00<00:00, 6629.35it/s]\n",
            "100%|██████████| 50/50 [00:00<00:00, 5746.41it/s]\n",
            "100%|██████████| 209/209 [00:00<00:00, 6395.06it/s]\n",
            "100%|██████████| 173/173 [00:00<00:00, 7908.61it/s]\n",
            "100%|██████████| 50/50 [00:00<00:00, 5756.50it/s]\n",
            "100%|██████████| 30/30 [00:00<00:00, 3603.87it/s]\n",
            "100%|██████████| 170/170 [00:00<00:00, 5486.38it/s]\n",
            "100%|██████████| 87/87 [00:00<00:00, 6155.50it/s]\n",
            "100%|██████████| 198/198 [00:00<00:00, 6860.97it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 7043.81it/s]\n",
            "100%|██████████| 114/114 [00:00<00:00, 7127.75it/s]\n",
            "100%|██████████| 109/109 [00:00<00:00, 7010.55it/s]\n",
            "100%|██████████| 238/238 [00:00<00:00, 7558.45it/s]\n",
            "100%|██████████| 207/207 [00:00<00:00, 7795.19it/s]\n",
            "100%|██████████| 119/119 [00:00<00:00, 7467.86it/s]\n",
            "100%|██████████| 133/133 [00:00<00:00, 7150.63it/s]\n",
            "100%|██████████| 108/108 [00:00<00:00, 7230.87it/s]\n",
            "100%|██████████| 121/121 [00:00<00:00, 5423.75it/s]\n",
            "100%|██████████| 87/87 [00:00<00:00, 4958.08it/s]\n",
            "100%|██████████| 185/185 [00:00<00:00, 7086.98it/s]\n",
            "100%|██████████| 265/265 [00:00<00:00, 7112.95it/s]\n",
            "100%|██████████| 116/116 [00:00<00:00, 7326.30it/s]\n",
            "100%|██████████| 72/72 [00:00<00:00, 5396.24it/s]\n",
            "100%|██████████| 197/197 [00:00<00:00, 7795.44it/s]\n",
            "100%|██████████| 82/82 [00:00<00:00, 6257.42it/s]\n",
            "100%|██████████| 206/206 [00:00<00:00, 7771.07it/s]\n",
            "100%|██████████| 68/68 [00:00<00:00, 6789.97it/s]\n",
            "100%|██████████| 117/117 [00:00<00:00, 6729.94it/s]\n",
            "100%|██████████| 354/354 [00:00<00:00, 7969.00it/s]\n",
            "100%|██████████| 64/64 [00:00<00:00, 6872.92it/s]\n",
            "100%|██████████| 53/53 [00:00<00:00, 6198.19it/s]\n",
            "100%|██████████| 377/377 [00:00<00:00, 7551.28it/s]\n",
            "100%|██████████| 42/42 [00:00<00:00, 4925.37it/s]\n",
            "100%|██████████| 138/138 [00:00<00:00, 5858.74it/s]\n",
            "100%|██████████| 67/67 [00:00<00:00, 6262.95it/s]\n",
            "100%|██████████| 50/50 [00:00<00:00, 5866.32it/s]\n",
            "100%|██████████| 291/291 [00:00<00:00, 7937.61it/s]\n",
            "100%|██████████| 149/149 [00:00<00:00, 7359.73it/s]\n",
            "100%|██████████| 32/32 [00:00<00:00, 4938.29it/s]\n",
            "100%|██████████| 92/92 [00:00<00:00, 6451.16it/s]\n",
            "100%|██████████| 126/126 [00:00<00:00, 6211.88it/s]\n",
            "100%|██████████| 206/206 [00:00<00:00, 6849.85it/s]\n",
            "100%|██████████| 97/97 [00:00<00:00, 6567.25it/s]\n",
            "100%|██████████| 380/380 [00:00<00:00, 7610.20it/s]\n",
            "100%|██████████| 169/169 [00:00<00:00, 7734.52it/s]\n",
            "100%|██████████| 212/212 [00:00<00:00, 7720.63it/s]\n",
            "100%|██████████| 114/114 [00:00<00:00, 7479.52it/s]\n",
            "100%|██████████| 140/140 [00:00<00:00, 6491.01it/s]\n",
            "100%|██████████| 79/79 [00:00<00:00, 6839.43it/s]\n",
            "100%|██████████| 249/249 [00:00<00:00, 7426.45it/s]\n",
            "100%|██████████| 116/116 [00:00<00:00, 6157.17it/s]\n",
            "100%|██████████| 230/230 [00:00<00:00, 7449.23it/s]\n",
            "100%|██████████| 111/111 [00:00<00:00, 7208.60it/s]\n",
            "100%|██████████| 395/395 [00:00<00:00, 7946.88it/s]\n",
            "100%|██████████| 42/42 [00:00<00:00, 6262.83it/s]\n",
            "100%|██████████| 80/80 [00:00<00:00, 5676.42it/s]\n",
            "100%|██████████| 58/58 [00:00<00:00, 7636.78it/s]\n",
            "100%|██████████| 320/320 [00:00<00:00, 7046.14it/s]\n",
            "100%|██████████| 93/93 [00:00<00:00, 5876.15it/s]\n",
            "100%|██████████| 30/30 [00:00<00:00, 4450.82it/s]\n",
            "100%|██████████| 46/46 [00:00<00:00, 5581.40it/s]\n",
            "100%|██████████| 117/117 [00:00<00:00, 6370.60it/s]\n",
            "100%|██████████| 62/62 [00:00<00:00, 7726.15it/s]\n",
            "100%|██████████| 118/118 [00:00<00:00, 7343.36it/s]\n",
            "100%|██████████| 144/144 [00:00<00:00, 5937.73it/s]\n",
            "100%|██████████| 147/147 [00:00<00:00, 6782.12it/s]\n",
            "100%|██████████| 36/36 [00:00<00:00, 7201.90it/s]\n",
            "100%|██████████| 199/199 [00:00<00:00, 7237.26it/s]\n",
            "100%|██████████| 116/116 [00:00<00:00, 5350.94it/s]\n",
            "100%|██████████| 76/76 [00:00<00:00, 5906.49it/s]\n",
            "100%|██████████| 75/75 [00:00<00:00, 5932.65it/s]\n",
            "100%|██████████| 202/202 [00:00<00:00, 6843.31it/s]\n",
            "100%|██████████| 127/127 [00:00<00:00, 6766.90it/s]\n",
            "100%|██████████| 40/40 [00:00<00:00, 6838.91it/s]\n",
            "100%|██████████| 270/270 [00:00<00:00, 7037.99it/s]\n",
            "100%|██████████| 95/95 [00:00<00:00, 6102.35it/s]\n",
            "100%|██████████| 133/133 [00:00<00:00, 6815.09it/s]\n",
            "100%|██████████| 49/49 [00:00<00:00, 4672.42it/s]\n",
            "100%|██████████| 146/146 [00:00<00:00, 7382.11it/s]\n",
            "100%|██████████| 124/124 [00:00<00:00, 7657.90it/s]\n",
            "100%|██████████| 191/191 [00:00<00:00, 4142.00it/s]\n",
            "100%|██████████| 183/183 [00:00<00:00, 6597.99it/s]\n",
            "100%|██████████| 113/113 [00:00<00:00, 6605.11it/s]\n",
            "100%|██████████| 154/154 [00:00<00:00, 3415.25it/s]\n",
            "100%|██████████| 256/256 [00:00<00:00, 3754.81it/s]\n",
            "100%|██████████| 130/130 [00:00<00:00, 6124.72it/s]\n",
            "100%|██████████| 138/138 [00:00<00:00, 5389.73it/s]\n",
            "100%|██████████| 35/35 [00:00<00:00, 2446.31it/s]\n",
            "100%|██████████| 138/138 [00:00<00:00, 2737.96it/s]\n",
            "100%|██████████| 174/174 [00:00<00:00, 3748.74it/s]\n",
            "100%|██████████| 114/114 [00:00<00:00, 2697.03it/s]\n",
            "100%|██████████| 66/66 [00:00<00:00, 3858.87it/s]\n",
            "100%|██████████| 186/186 [00:00<00:00, 3419.15it/s]\n",
            "100%|██████████| 28/28 [00:00<00:00, 2751.07it/s]\n",
            "100%|██████████| 148/148 [00:00<00:00, 1839.75it/s]\n",
            "100%|██████████| 147/147 [00:00<00:00, 2494.17it/s]\n",
            "100%|██████████| 160/160 [00:00<00:00, 2266.82it/s]\n",
            "100%|██████████| 142/142 [00:00<00:00, 4089.05it/s]\n",
            "100%|██████████| 135/135 [00:00<00:00, 4475.96it/s]\n",
            "100%|██████████| 265/265 [00:00<00:00, 5364.47it/s]\n",
            "100%|██████████| 63/63 [00:00<00:00, 4057.82it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 3381.96it/s]\n",
            "100%|██████████| 131/131 [00:00<00:00, 4665.64it/s]\n",
            "100%|██████████| 407/407 [00:00<00:00, 3453.66it/s]\n",
            "100%|██████████| 160/160 [00:00<00:00, 3562.10it/s]\n",
            "100%|██████████| 211/211 [00:00<00:00, 2953.31it/s]\n",
            "100%|██████████| 126/126 [00:00<00:00, 5209.85it/s]\n",
            "100%|██████████| 138/138 [00:00<00:00, 4947.17it/s]\n",
            "100%|██████████| 92/92 [00:00<00:00, 3158.88it/s]\n",
            "100%|██████████| 67/67 [00:00<00:00, 2295.26it/s]\n",
            "100%|██████████| 106/106 [00:00<00:00, 3201.48it/s]\n",
            "100%|██████████| 175/175 [00:00<00:00, 3448.29it/s]\n",
            "100%|██████████| 68/68 [00:00<00:00, 3620.46it/s]\n",
            "100%|██████████| 103/103 [00:00<00:00, 3820.56it/s]\n",
            "100%|██████████| 153/153 [00:00<00:00, 2976.50it/s]\n",
            "100%|██████████| 209/209 [00:00<00:00, 3079.06it/s]\n",
            "100%|██████████| 163/163 [00:00<00:00, 3273.36it/s]\n",
            "100%|██████████| 173/173 [00:00<00:00, 2125.87it/s]\n",
            "100%|██████████| 207/207 [00:00<00:00, 2059.22it/s]\n",
            "100%|██████████| 128/128 [00:00<00:00, 2910.07it/s]\n",
            "100%|██████████| 141/141 [00:00<00:00, 3253.87it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 3046.12it/s]\n",
            "100%|██████████| 125/125 [00:00<00:00, 4661.37it/s]\n",
            "100%|██████████| 62/62 [00:00<00:00, 3131.25it/s]\n",
            "100%|██████████| 70/70 [00:00<00:00, 3167.36it/s]\n",
            "100%|██████████| 33/33 [00:00<00:00, 5346.78it/s]\n",
            "100%|██████████| 49/49 [00:00<00:00, 3710.43it/s]\n",
            "100%|██████████| 280/280 [00:00<00:00, 4184.43it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 4846.26it/s]\n",
            "100%|██████████| 173/173 [00:00<00:00, 4272.77it/s]\n",
            "100%|██████████| 161/161 [00:00<00:00, 3925.72it/s]\n",
            "100%|██████████| 79/79 [00:00<00:00, 3969.59it/s]\n",
            "100%|██████████| 59/59 [00:00<00:00, 4039.30it/s]\n",
            "100%|██████████| 124/124 [00:00<00:00, 3821.91it/s]\n",
            "100%|██████████| 134/134 [00:00<00:00, 3025.36it/s]\n",
            "100%|██████████| 46/46 [00:00<00:00, 2306.41it/s]\n",
            "100%|██████████| 188/188 [00:00<00:00, 4146.23it/s]\n",
            "100%|██████████| 76/76 [00:00<00:00, 4517.87it/s]\n",
            "100%|██████████| 146/146 [00:00<00:00, 2755.04it/s]\n",
            "100%|██████████| 143/143 [00:00<00:00, 1459.93it/s]\n",
            "100%|██████████| 253/253 [00:00<00:00, 2012.79it/s]\n",
            "100%|██████████| 109/109 [00:00<00:00, 2241.92it/s]\n",
            "100%|██████████| 125/125 [00:00<00:00, 1944.46it/s]\n",
            "100%|██████████| 381/381 [00:00<00:00, 1782.17it/s]\n",
            "100%|██████████| 89/89 [00:00<00:00, 1517.40it/s]\n",
            "100%|██████████| 195/195 [00:00<00:00, 1480.82it/s]\n",
            "100%|██████████| 208/208 [00:00<00:00, 1232.91it/s]\n",
            "100%|██████████| 162/162 [00:00<00:00, 1201.65it/s]\n",
            "100%|██████████| 95/95 [00:00<00:00, 1873.37it/s]\n",
            "100%|██████████| 177/177 [00:00<00:00, 1254.54it/s]\n",
            "100%|██████████| 103/103 [00:00<00:00, 1372.51it/s]\n",
            "100%|██████████| 82/82 [00:00<00:00, 1641.20it/s]\n",
            "100%|██████████| 94/94 [00:00<00:00, 1959.74it/s]\n",
            "100%|██████████| 145/145 [00:00<00:00, 2386.85it/s]\n",
            "100%|██████████| 163/163 [00:00<00:00, 2266.73it/s]\n",
            "100%|██████████| 137/137 [00:00<00:00, 2607.75it/s]\n",
            "100%|██████████| 42/42 [00:00<00:00, 2188.55it/s]\n",
            "100%|██████████| 228/228 [00:00<00:00, 2592.59it/s]\n",
            "100%|██████████| 133/133 [00:00<00:00, 2666.00it/s]\n",
            "100%|██████████| 167/167 [00:00<00:00, 2926.33it/s]\n",
            "100%|██████████| 23/23 [00:00<00:00, 1093.91it/s]\n",
            "100%|██████████| 119/119 [00:00<00:00, 2564.18it/s]\n",
            "100%|██████████| 78/78 [00:00<00:00, 2835.39it/s]\n",
            "100%|██████████| 269/269 [00:00<00:00, 1819.83it/s]\n",
            "100%|██████████| 102/102 [00:00<00:00, 2671.25it/s]\n",
            "100%|██████████| 76/76 [00:00<00:00, 3220.10it/s]\n",
            "100%|██████████| 141/141 [00:00<00:00, 5906.17it/s]\n",
            "100%|██████████| 299/299 [00:00<00:00, 5193.33it/s]\n",
            "100%|██████████| 84/84 [00:00<00:00, 4991.03it/s]\n",
            "100%|██████████| 211/211 [00:00<00:00, 1330.40it/s]\n",
            "100%|██████████| 103/103 [00:00<00:00, 1261.21it/s]\n",
            "100%|██████████| 52/52 [00:00<00:00, 1549.20it/s]\n",
            "100%|██████████| 95/95 [00:00<00:00, 1613.93it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 1376.29it/s]\n",
            "100%|██████████| 177/177 [00:00<00:00, 2142.59it/s]\n",
            "100%|██████████| 45/45 [00:00<00:00, 2340.02it/s]\n",
            "100%|██████████| 56/56 [00:00<00:00, 2247.60it/s]\n",
            "100%|██████████| 51/51 [00:00<00:00, 3834.53it/s]\n",
            "100%|██████████| 43/43 [00:00<00:00, 3672.55it/s]\n",
            "100%|██████████| 253/253 [00:00<00:00, 7114.76it/s]\n",
            "100%|██████████| 127/127 [00:00<00:00, 3679.52it/s]\n",
            "100%|██████████| 272/272 [00:00<00:00, 3818.79it/s]\n",
            "100%|██████████| 228/228 [00:00<00:00, 5526.54it/s]\n",
            "100%|██████████| 60/60 [00:00<00:00, 7063.69it/s]\n",
            "100%|██████████| 130/130 [00:00<00:00, 5876.27it/s]\n",
            "100%|██████████| 127/127 [00:00<00:00, 3859.56it/s]\n",
            "100%|██████████| 32/32 [00:00<00:00, 2278.89it/s]\n",
            "100%|██████████| 160/160 [00:00<00:00, 3780.89it/s]\n",
            "100%|██████████| 102/102 [00:00<00:00, 3013.06it/s]\n",
            "100%|██████████| 100/100 [00:00<00:00, 2942.52it/s]\n",
            "100%|██████████| 199/199 [00:00<00:00, 3233.56it/s]\n",
            "100%|██████████| 33/33 [00:00<00:00, 2375.97it/s]\n",
            "100%|██████████| 49/49 [00:00<00:00, 4374.37it/s]\n",
            "100%|██████████| 127/127 [00:00<00:00, 5737.14it/s]\n",
            "100%|██████████| 75/75 [00:00<00:00, 9837.16it/s]\n",
            "100%|██████████| 130/130 [00:00<00:00, 10367.53it/s]\n",
            "100%|██████████| 120/120 [00:00<00:00, 6428.63it/s]\n",
            "100%|██████████| 150/150 [00:00<00:00, 9738.49it/s]\n",
            "100%|██████████| 57/57 [00:00<00:00, 7143.40it/s]\n",
            "100%|██████████| 74/74 [00:00<00:00, 7278.70it/s]\n",
            "100%|██████████| 192/192 [00:00<00:00, 9601.04it/s]\n",
            "100%|██████████| 207/207 [00:00<00:00, 10824.75it/s]\n",
            "100%|██████████| 246/246 [00:00<00:00, 10455.27it/s]\n",
            "100%|██████████| 92/92 [00:00<00:00, 9266.06it/s]\n",
            "100%|██████████| 96/96 [00:00<00:00, 7914.56it/s]\n",
            "100%|██████████| 62/62 [00:00<00:00, 6988.82it/s]\n",
            "100%|██████████| 298/298 [00:00<00:00, 11533.55it/s]\n",
            "100%|██████████| 165/165 [00:00<00:00, 9788.96it/s]\n",
            "100%|██████████| 177/177 [00:00<00:00, 10012.16it/s]\n",
            "100%|██████████| 165/165 [00:00<00:00, 7992.01it/s]\n",
            "100%|██████████| 131/131 [00:00<00:00, 9826.42it/s]\n",
            "100%|██████████| 209/209 [00:00<00:00, 9781.62it/s]\n",
            "100%|██████████| 56/56 [00:00<00:00, 6974.32it/s]\n",
            "100%|██████████| 80/80 [00:00<00:00, 7146.39it/s]\n",
            "100%|██████████| 94/94 [00:00<00:00, 7861.08it/s]\n",
            "100%|██████████| 97/97 [00:00<00:00, 8532.88it/s]\n",
            "100%|██████████| 143/143 [00:00<00:00, 9767.38it/s]\n",
            "100%|██████████| 68/68 [00:00<00:00, 8043.45it/s]\n",
            "100%|██████████| 265/265 [00:00<00:00, 8354.18it/s]\n",
            "100%|██████████| 117/117 [00:00<00:00, 8416.81it/s]\n",
            "100%|██████████| 191/191 [00:00<00:00, 8766.44it/s]\n",
            "100%|██████████| 27/27 [00:00<00:00, 6123.07it/s]\n",
            "100%|██████████| 180/180 [00:00<00:00, 8237.59it/s]\n",
            "100%|██████████| 137/137 [00:00<00:00, 9247.92it/s]\n",
            "100%|██████████| 35/35 [00:00<00:00, 5954.68it/s]\n",
            "100%|██████████| 58/58 [00:00<00:00, 7097.79it/s]\n",
            "100%|██████████| 199/199 [00:00<00:00, 8685.40it/s]\n",
            "100%|██████████| 343/343 [00:00<00:00, 11975.15it/s]\n",
            "100%|██████████| 155/155 [00:00<00:00, 10460.28it/s]\n",
            "100%|██████████| 161/161 [00:00<00:00, 9846.21it/s]\n",
            "100%|██████████| 113/113 [00:00<00:00, 5169.46it/s]\n",
            "100%|██████████| 145/145 [00:00<00:00, 6726.03it/s]\n",
            "100%|██████████| 69/69 [00:00<00:00, 7556.52it/s]\n",
            "100%|██████████| 26/26 [00:00<00:00, 5152.46it/s]\n",
            "100%|██████████| 272/272 [00:00<00:00, 10334.17it/s]\n",
            "100%|██████████| 173/173 [00:00<00:00, 10990.16it/s]\n",
            "100%|██████████| 120/120 [00:00<00:00, 8764.46it/s]\n",
            "100%|██████████| 257/257 [00:00<00:00, 10705.92it/s]\n",
            "100%|██████████| 300/300 [00:00<00:00, 11526.61it/s]\n",
            "100%|██████████| 205/205 [00:00<00:00, 10164.22it/s]\n",
            "100%|██████████| 301/301 [00:00<00:00, 10716.92it/s]\n",
            "100%|██████████| 115/115 [00:00<00:00, 8574.87it/s]\n",
            "100%|██████████| 191/191 [00:00<00:00, 10310.06it/s]\n",
            "100%|██████████| 48/48 [00:00<00:00, 7398.18it/s]\n",
            "100%|██████████| 148/148 [00:00<00:00, 8201.96it/s]\n",
            "100%|██████████| 80/80 [00:00<00:00, 6307.46it/s]\n",
            "100%|██████████| 32/32 [00:00<00:00, 6396.80it/s]\n",
            "100%|██████████| 26/26 [00:00<00:00, 7314.50it/s]\n",
            "100%|██████████| 31/31 [00:00<00:00, 7263.47it/s]\n",
            "100%|██████████| 156/156 [00:00<00:00, 6892.57it/s]\n",
            "100%|██████████| 89/89 [00:00<00:00, 7736.00it/s]\n",
            "100%|██████████| 168/168 [00:00<00:00, 10103.28it/s]\n",
            "100%|██████████| 69/69 [00:00<00:00, 7137.22it/s]\n",
            "100%|██████████| 98/98 [00:00<00:00, 8014.85it/s]\n",
            "100%|██████████| 65/65 [00:00<00:00, 7137.65it/s]\n",
            "100%|██████████| 60/60 [00:00<00:00, 6843.19it/s]\n",
            "100%|██████████| 141/141 [00:00<00:00, 7766.62it/s]\n",
            "100%|██████████| 169/169 [00:00<00:00, 8750.43it/s]\n",
            "100%|██████████| 214/214 [00:00<00:00, 10042.08it/s]\n",
            "100%|██████████| 139/139 [00:00<00:00, 8897.22it/s]\n",
            "100%|██████████| 62/62 [00:00<00:00, 6293.49it/s]\n",
            "100%|██████████| 18/18 [00:00<00:00, 5822.73it/s]\n",
            "100%|██████████| 162/162 [00:00<00:00, 8162.48it/s]\n",
            "100%|██████████| 632/632 [00:00<00:00, 12138.31it/s]\n",
            "100%|██████████| 104/104 [00:00<00:00, 7752.05it/s]\n",
            "100%|██████████| 46/46 [00:00<00:00, 6894.83it/s]\n",
            "100%|██████████| 177/177 [00:00<00:00, 10207.79it/s]\n",
            "100%|██████████| 238/238 [00:00<00:00, 9893.01it/s]\n",
            "100%|██████████| 89/89 [00:00<00:00, 7975.32it/s]\n",
            "100%|██████████| 363/363 [00:00<00:00, 10716.63it/s]\n",
            "100%|██████████| 80/80 [00:00<00:00, 8162.11it/s]\n",
            "100%|██████████| 105/105 [00:00<00:00, 8572.80it/s]\n",
            "100%|██████████| 103/103 [00:00<00:00, 6587.07it/s]\n",
            "100%|██████████| 1423/1423 [00:00<00:00, 11518.48it/s]\n",
            "100%|██████████| 158/158 [00:00<00:00, 6968.38it/s]\n",
            "100%|██████████| 110/110 [00:00<00:00, 10017.01it/s]\n",
            "100%|██████████| 46/46 [00:00<00:00, 7357.59it/s]\n",
            "100%|██████████| 182/182 [00:00<00:00, 8848.02it/s]\n",
            "100%|██████████| 48/48 [00:00<00:00, 6692.15it/s]\n",
            "100%|██████████| 21/21 [00:00<00:00, 6320.35it/s]\n",
            "100%|██████████| 663/663 [00:00<00:00, 11991.43it/s]\n",
            "100%|██████████| 126/126 [00:00<00:00, 9476.95it/s]\n",
            "100%|██████████| 44/44 [00:00<00:00, 6906.01it/s]\n",
            "100%|██████████| 131/131 [00:00<00:00, 7340.05it/s]\n",
            "100%|██████████| 98/98 [00:00<00:00, 8219.69it/s]\n",
            "100%|██████████| 91/91 [00:00<00:00, 7206.44it/s]\n",
            "100%|██████████| 458/458 [00:00<00:00, 11504.53it/s]\n",
            "100%|██████████| 28/28 [00:00<00:00, 5473.81it/s]\n",
            "100%|██████████| 54/54 [00:00<00:00, 6478.99it/s]\n",
            "100%|██████████| 59/59 [00:00<00:00, 6834.13it/s]\n",
            "100%|██████████| 583/583 [00:00<00:00, 12019.60it/s]\n",
            "100%|██████████| 174/174 [00:00<00:00, 9527.90it/s]\n",
            "100%|██████████| 91/91 [00:00<00:00, 8638.46it/s]\n",
            "100%|██████████| 42/42 [00:00<00:00, 7745.71it/s]\n",
            "100%|██████████| 151/151 [00:00<00:00, 8061.25it/s]\n",
            "100%|██████████| 96/96 [00:00<00:00, 7294.57it/s]\n",
            "100%|██████████| 328/328 [00:00<00:00, 10083.72it/s]\n",
            "100%|██████████| 73/73 [00:00<00:00, 6942.17it/s]\n",
            "100%|██████████| 93/93 [00:00<00:00, 7521.02it/s]\n",
            "100%|██████████| 34/34 [00:00<00:00, 6155.05it/s]\n",
            "100%|██████████| 136/136 [00:00<00:00, 7833.15it/s]\n",
            "100%|██████████| 72/72 [00:00<00:00, 7329.50it/s]\n",
            "100%|██████████| 197/197 [00:00<00:00, 9508.05it/s]\n",
            "100%|██████████| 120/120 [00:00<00:00, 8600.03it/s]\n",
            "100%|██████████| 187/187 [00:00<00:00, 9653.59it/s]\n",
            "100%|██████████| 78/78 [00:00<00:00, 6972.48it/s]\n",
            "100%|██████████| 390/390 [00:00<00:00, 11412.12it/s]\n",
            "100%|██████████| 152/152 [00:00<00:00, 9428.61it/s]\n",
            "100%|██████████| 263/263 [00:00<00:00, 10092.98it/s]\n",
            "100%|██████████| 42/42 [00:00<00:00, 6369.48it/s]\n",
            "100%|██████████| 225/225 [00:00<00:00, 10071.92it/s]\n",
            "100%|██████████| 130/130 [00:00<00:00, 7981.32it/s]\n",
            "100%|██████████| 60/60 [00:00<00:00, 7020.54it/s]\n",
            "100%|██████████| 109/109 [00:00<00:00, 7315.92it/s]\n",
            "100%|██████████| 97/97 [00:00<00:00, 6988.35it/s]\n",
            "100%|██████████| 84/84 [00:00<00:00, 8360.54it/s]\n",
            "100%|██████████| 97/97 [00:00<00:00, 8095.82it/s]\n",
            "100%|██████████| 159/159 [00:00<00:00, 9060.08it/s]\n",
            "100%|██████████| 84/84 [00:00<00:00, 7428.71it/s]\n",
            "100%|██████████| 164/164 [00:00<00:00, 9356.43it/s]\n",
            "100%|██████████| 220/220 [00:00<00:00, 6616.19it/s]\n",
            "100%|██████████| 91/91 [00:00<00:00, 7521.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8043912175648703\n",
            "Recall: 0.5623725055432373\n",
            "Precision: 0.5445123109057536\n",
            "F1: 0.5490300536409729\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "\n",
        "# Đường dẫn đến tệp dữ liệu test\n",
        "test_file = \"s-data.json\"\n",
        "\n",
        "# Load test data\n",
        "with open(test_file, \"r\") as f:\n",
        "    test_data = [json.loads(line) for line in f]\n",
        "\n",
        "phoBERT_cls.register_classification_head('new_task', num_classes=2)\n",
        "phoBERT_cls = phoBERT_cls.to('cuda')\n",
        "\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "convert_text_int = []\n",
        "for example in test_data:\n",
        "    text = example['sentence']\n",
        "    label = example['labels']\n",
        "\n",
        "    encoded = convert_lines(text, vocab, phoBERT.bpe)  # Sử dụng [text] thay vì text\n",
        "    convert_text_int.append(encoded[0])  # Thêm mã hóa của từng văn bản vào danh sách\n",
        "\n",
        "my_tensor = torch.tensor(convert_text_int)\n",
        "my_tensor = my_tensor.to('cuda')\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = phoBERT_cls.predict('new_task', my_tensor)\n",
        "\n",
        "# Get predicted labels\n",
        "pred_labels = torch.argmax(logits, dim=1).tolist()\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "true_labels = [example['labels'] for example in test_data]\n",
        "accuracy = accuracy_score(true_labels, pred_labels)\n",
        "recall = recall_score(true_labels, pred_labels, average='macro')\n",
        "precision = precision_score(true_labels, pred_labels, average='macro')\n",
        "f1 = f1_score(true_labels, pred_labels, average='macro')\n",
        "\n",
        "# Print evaluation metrics\n",
        "print('Accuracy:', accuracy)\n",
        "print('Recall:', recall)\n",
        "print('Precision:', precision)\n",
        "print('F1:', f1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Chuỗi câu bạn muốn dự đoán\n",
        "text = sentences[0]\n",
        "\n",
        "# Tiền xử lý dữ liệu: encoded là đầu vào được mã hóa của câu\n",
        "encoded = phoBERT.encode(text)\n",
        "\n",
        "# Chuyển đổi dữ liệu thành tensor PyTorch\n",
        "input_tensor = torch.tensor(encoded)\n",
        "\n",
        "# Đảm bảo rằng mô hình và dữ liệu đều ở chế độ \"eval\" (đánh giá)\n",
        "phoBERT_cls.eval()\n",
        "\n",
        "# Tính toán dự đoán với torch.no_grad() để tắt tính toán gradient\n",
        "with torch.no_grad():\n",
        "    # Truyền dữ liệu qua mô hình để nhận logits\n",
        "    logits = phoBERT_cls.predict('new_task', input_tensor)\n",
        "import numpy as np\n",
        "with torch.no_grad():\n",
        "  x = np.argmax(logits, axis = 1)\n",
        "  print(x)"
      ],
      "metadata": {
        "id": "6Rof-jw55wr4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}